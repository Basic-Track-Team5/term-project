{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The RT-IoT2022, a proprietary dataset derived from a **real-time IoT infrastructure**, is introduced as a comprehensive resource integrating a diverse range of IoT devices and **sophisticated network attack methodologies**. This dataset encompasses both **normal** and **adversarial** network behaviours, providing a general representation of real-world scenarios. Incorporating data from IoT devices such as ThingSpeak-LED, Wipro-Bulb, and MQTT-Temp, as well as simulated attack scenarios involving Brute-Force SSH attacks, DDoS attacks using Hping and Slowloris, and Nmap patterns, RT-IoT2022 offers a **detailed perspective on the complex nature of network traffic**. The bidirectional attributes of network traffic are meticulously captured using the Zeek network monitoring tool and the Flowmeter plugin. Researchers can leverage the RT-IoT2022 dataset to advance the capabilities of Intrusion Detection Systems (IDS), fostering the development of robust and adaptive security solutions for real-time IoT networks.\n",
    "\n",
    "\n",
    "Class Labels\n",
    "\n",
    "The Dataset contains both Attack patterns and Normal Patterns. \n",
    "\n",
    "Attacks patterns Details: \n",
    "1.\tDOS_SYN_Hping------------------------94659\n",
    "2.\tARP_poisioning--------------------------7750\n",
    "3.\tNMAP_UDP_SCAN--------------------2590\n",
    "4.\tNMAP_XMAS_TREE_SCAN--------2010\n",
    "5.\tNMAP_OS_DETECTION-------------2000\n",
    "6.\tNMAP_TCP_scan-----------------------1002\n",
    "7.\tDDOS_Slowloris------------------------534\n",
    "8.\tMetasploit_Brute_Force_SSH---------37\n",
    "9.\tNMAP_FIN_SCAN---------------------28\n",
    "\n",
    "<br>\n",
    "\n",
    "Normal Patterns Details:\n",
    "\n",
    "1.\tMQTT -----------------------------------8108\n",
    "2.\tThing_speak-----------------------------4146\n",
    "3.\tWipro_bulb_Dataset-------------------253 \n",
    "4. Amazon-Alexa -----------------------86842 (Amazon Alexa is missing in the current dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 한글 폰트 출력 관련 (필요시 주석 해제 후 사용)\n",
    "# import matplotlib\n",
    "# print(matplotlib.get_data_path())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 폰트 설정 (https://github.com/Basic-Track-Team5/term-project/issues/2 폰트 설치가 되지 않았다면 먼저 진행)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rc('font', family=['NanumGothic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1. Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id.orig_p</th>\n",
       "      <th>id.resp_p</th>\n",
       "      <th>proto</th>\n",
       "      <th>service</th>\n",
       "      <th>flow_duration</th>\n",
       "      <th>fwd_pkts_tot</th>\n",
       "      <th>bwd_pkts_tot</th>\n",
       "      <th>fwd_data_pkts_tot</th>\n",
       "      <th>bwd_data_pkts_tot</th>\n",
       "      <th>fwd_pkts_per_sec</th>\n",
       "      <th>bwd_pkts_per_sec</th>\n",
       "      <th>flow_pkts_per_sec</th>\n",
       "      <th>down_up_ratio</th>\n",
       "      <th>fwd_header_size_tot</th>\n",
       "      <th>fwd_header_size_min</th>\n",
       "      <th>fwd_header_size_max</th>\n",
       "      <th>bwd_header_size_tot</th>\n",
       "      <th>bwd_header_size_min</th>\n",
       "      <th>bwd_header_size_max</th>\n",
       "      <th>flow_FIN_flag_count</th>\n",
       "      <th>flow_SYN_flag_count</th>\n",
       "      <th>flow_RST_flag_count</th>\n",
       "      <th>fwd_PSH_flag_count</th>\n",
       "      <th>bwd_PSH_flag_count</th>\n",
       "      <th>flow_ACK_flag_count</th>\n",
       "      <th>fwd_URG_flag_count</th>\n",
       "      <th>bwd_URG_flag_count</th>\n",
       "      <th>flow_CWR_flag_count</th>\n",
       "      <th>flow_ECE_flag_count</th>\n",
       "      <th>fwd_pkts_payload.min</th>\n",
       "      <th>fwd_pkts_payload.max</th>\n",
       "      <th>fwd_pkts_payload.tot</th>\n",
       "      <th>fwd_pkts_payload.avg</th>\n",
       "      <th>fwd_pkts_payload.std</th>\n",
       "      <th>bwd_pkts_payload.min</th>\n",
       "      <th>bwd_pkts_payload.max</th>\n",
       "      <th>bwd_pkts_payload.tot</th>\n",
       "      <th>bwd_pkts_payload.avg</th>\n",
       "      <th>bwd_pkts_payload.std</th>\n",
       "      <th>flow_pkts_payload.min</th>\n",
       "      <th>flow_pkts_payload.max</th>\n",
       "      <th>flow_pkts_payload.tot</th>\n",
       "      <th>flow_pkts_payload.avg</th>\n",
       "      <th>flow_pkts_payload.std</th>\n",
       "      <th>fwd_iat.min</th>\n",
       "      <th>fwd_iat.max</th>\n",
       "      <th>fwd_iat.tot</th>\n",
       "      <th>fwd_iat.avg</th>\n",
       "      <th>fwd_iat.std</th>\n",
       "      <th>bwd_iat.min</th>\n",
       "      <th>bwd_iat.max</th>\n",
       "      <th>bwd_iat.tot</th>\n",
       "      <th>bwd_iat.avg</th>\n",
       "      <th>bwd_iat.std</th>\n",
       "      <th>flow_iat.min</th>\n",
       "      <th>flow_iat.max</th>\n",
       "      <th>flow_iat.tot</th>\n",
       "      <th>flow_iat.avg</th>\n",
       "      <th>flow_iat.std</th>\n",
       "      <th>payload_bytes_per_second</th>\n",
       "      <th>fwd_subflow_pkts</th>\n",
       "      <th>bwd_subflow_pkts</th>\n",
       "      <th>fwd_subflow_bytes</th>\n",
       "      <th>bwd_subflow_bytes</th>\n",
       "      <th>fwd_bulk_bytes</th>\n",
       "      <th>bwd_bulk_bytes</th>\n",
       "      <th>fwd_bulk_packets</th>\n",
       "      <th>bwd_bulk_packets</th>\n",
       "      <th>fwd_bulk_rate</th>\n",
       "      <th>bwd_bulk_rate</th>\n",
       "      <th>active.min</th>\n",
       "      <th>active.max</th>\n",
       "      <th>active.tot</th>\n",
       "      <th>active.avg</th>\n",
       "      <th>active.std</th>\n",
       "      <th>idle.min</th>\n",
       "      <th>idle.max</th>\n",
       "      <th>idle.tot</th>\n",
       "      <th>idle.avg</th>\n",
       "      <th>idle.std</th>\n",
       "      <th>fwd_init_window_size</th>\n",
       "      <th>bwd_init_window_size</th>\n",
       "      <th>fwd_last_window_size</th>\n",
       "      <th>Attack_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>38667</td>\n",
       "      <td>1883</td>\n",
       "      <td>tcp</td>\n",
       "      <td>mqtt</td>\n",
       "      <td>32.011598</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.281148</td>\n",
       "      <td>0.156193</td>\n",
       "      <td>0.437341</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>296</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>168</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>8.444444</td>\n",
       "      <td>13.115936</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>6.4</td>\n",
       "      <td>9.555103</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>7.714286</td>\n",
       "      <td>11.618477</td>\n",
       "      <td>761.985779</td>\n",
       "      <td>2.972918e+07</td>\n",
       "      <td>3.201160e+07</td>\n",
       "      <td>4.001450e+06</td>\n",
       "      <td>1.040307e+07</td>\n",
       "      <td>4438.877106</td>\n",
       "      <td>1.511694e+06</td>\n",
       "      <td>2.026391e+06</td>\n",
       "      <td>506597.757339</td>\n",
       "      <td>680406.147126</td>\n",
       "      <td>761.985779</td>\n",
       "      <td>2.972918e+07</td>\n",
       "      <td>3.201160e+07</td>\n",
       "      <td>2.462431e+06</td>\n",
       "      <td>8.199747e+06</td>\n",
       "      <td>3.373777</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>25.333333</td>\n",
       "      <td>10.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.282415e+06</td>\n",
       "      <td>2.282415e+06</td>\n",
       "      <td>2.282415e+06</td>\n",
       "      <td>2.282415e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.972918e+07</td>\n",
       "      <td>2.972918e+07</td>\n",
       "      <td>2.972918e+07</td>\n",
       "      <td>2.972918e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64240</td>\n",
       "      <td>26847</td>\n",
       "      <td>502</td>\n",
       "      <td>MQTT_Publish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>51143</td>\n",
       "      <td>1883</td>\n",
       "      <td>tcp</td>\n",
       "      <td>mqtt</td>\n",
       "      <td>31.883584</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.282277</td>\n",
       "      <td>0.156821</td>\n",
       "      <td>0.439097</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>296</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>168</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>8.444444</td>\n",
       "      <td>13.115936</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>6.4</td>\n",
       "      <td>9.555103</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>7.714286</td>\n",
       "      <td>11.618477</td>\n",
       "      <td>247.001648</td>\n",
       "      <td>2.985528e+07</td>\n",
       "      <td>3.188358e+07</td>\n",
       "      <td>3.985448e+06</td>\n",
       "      <td>1.046346e+07</td>\n",
       "      <td>4214.048386</td>\n",
       "      <td>1.576436e+06</td>\n",
       "      <td>1.876261e+06</td>\n",
       "      <td>469065.248966</td>\n",
       "      <td>741351.686212</td>\n",
       "      <td>247.001648</td>\n",
       "      <td>2.985528e+07</td>\n",
       "      <td>3.188358e+07</td>\n",
       "      <td>2.452583e+06</td>\n",
       "      <td>8.242459e+06</td>\n",
       "      <td>3.387323</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>25.333333</td>\n",
       "      <td>10.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.028307e+06</td>\n",
       "      <td>2.028307e+06</td>\n",
       "      <td>2.028307e+06</td>\n",
       "      <td>2.028307e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.985528e+07</td>\n",
       "      <td>2.985528e+07</td>\n",
       "      <td>2.985528e+07</td>\n",
       "      <td>2.985528e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64240</td>\n",
       "      <td>26847</td>\n",
       "      <td>502</td>\n",
       "      <td>MQTT_Publish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>44761</td>\n",
       "      <td>1883</td>\n",
       "      <td>tcp</td>\n",
       "      <td>mqtt</td>\n",
       "      <td>32.124053</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.280164</td>\n",
       "      <td>0.155647</td>\n",
       "      <td>0.435811</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>296</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>168</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>8.222222</td>\n",
       "      <td>12.852799</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.689074</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>7.428571</td>\n",
       "      <td>11.229866</td>\n",
       "      <td>283.956528</td>\n",
       "      <td>2.984215e+07</td>\n",
       "      <td>3.212405e+07</td>\n",
       "      <td>4.015507e+06</td>\n",
       "      <td>1.044238e+07</td>\n",
       "      <td>2456.903458</td>\n",
       "      <td>1.476049e+06</td>\n",
       "      <td>2.013770e+06</td>\n",
       "      <td>503442.466259</td>\n",
       "      <td>660344.360027</td>\n",
       "      <td>283.956528</td>\n",
       "      <td>2.984215e+07</td>\n",
       "      <td>3.212405e+07</td>\n",
       "      <td>2.471081e+06</td>\n",
       "      <td>8.230593e+06</td>\n",
       "      <td>3.237450</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>24.666667</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.281904e+06</td>\n",
       "      <td>2.281904e+06</td>\n",
       "      <td>2.281904e+06</td>\n",
       "      <td>2.281904e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.984215e+07</td>\n",
       "      <td>2.984215e+07</td>\n",
       "      <td>2.984215e+07</td>\n",
       "      <td>2.984215e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64240</td>\n",
       "      <td>26847</td>\n",
       "      <td>502</td>\n",
       "      <td>MQTT_Publish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>60893</td>\n",
       "      <td>1883</td>\n",
       "      <td>tcp</td>\n",
       "      <td>mqtt</td>\n",
       "      <td>31.961063</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.281593</td>\n",
       "      <td>0.156440</td>\n",
       "      <td>0.438033</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>296</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>168</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>8.222222</td>\n",
       "      <td>12.852799</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.689074</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>7.428571</td>\n",
       "      <td>11.229866</td>\n",
       "      <td>288.963318</td>\n",
       "      <td>2.991377e+07</td>\n",
       "      <td>3.196106e+07</td>\n",
       "      <td>3.995133e+06</td>\n",
       "      <td>1.048253e+07</td>\n",
       "      <td>3933.906555</td>\n",
       "      <td>1.551892e+06</td>\n",
       "      <td>1.883784e+06</td>\n",
       "      <td>470946.013927</td>\n",
       "      <td>724569.317911</td>\n",
       "      <td>288.963318</td>\n",
       "      <td>2.991377e+07</td>\n",
       "      <td>3.196106e+07</td>\n",
       "      <td>2.458543e+06</td>\n",
       "      <td>8.257786e+06</td>\n",
       "      <td>3.253959</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>24.666667</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.047288e+06</td>\n",
       "      <td>2.047288e+06</td>\n",
       "      <td>2.047288e+06</td>\n",
       "      <td>2.047288e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.991377e+07</td>\n",
       "      <td>2.991377e+07</td>\n",
       "      <td>2.991377e+07</td>\n",
       "      <td>2.991377e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64240</td>\n",
       "      <td>26847</td>\n",
       "      <td>502</td>\n",
       "      <td>MQTT_Publish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>51087</td>\n",
       "      <td>1883</td>\n",
       "      <td>tcp</td>\n",
       "      <td>mqtt</td>\n",
       "      <td>31.902362</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.282111</td>\n",
       "      <td>0.156728</td>\n",
       "      <td>0.438839</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>296</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>168</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>8.444444</td>\n",
       "      <td>13.115936</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>6.4</td>\n",
       "      <td>9.555103</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>7.714286</td>\n",
       "      <td>11.618477</td>\n",
       "      <td>387.907028</td>\n",
       "      <td>2.981470e+07</td>\n",
       "      <td>3.190236e+07</td>\n",
       "      <td>3.987795e+06</td>\n",
       "      <td>1.044702e+07</td>\n",
       "      <td>3005.027771</td>\n",
       "      <td>1.632083e+06</td>\n",
       "      <td>1.935984e+06</td>\n",
       "      <td>483996.033669</td>\n",
       "      <td>768543.390313</td>\n",
       "      <td>387.907028</td>\n",
       "      <td>2.981470e+07</td>\n",
       "      <td>3.190236e+07</td>\n",
       "      <td>2.454028e+06</td>\n",
       "      <td>8.230584e+06</td>\n",
       "      <td>3.385329</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>25.333333</td>\n",
       "      <td>10.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.087657e+06</td>\n",
       "      <td>2.087657e+06</td>\n",
       "      <td>2.087657e+06</td>\n",
       "      <td>2.087657e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.981470e+07</td>\n",
       "      <td>2.981470e+07</td>\n",
       "      <td>2.981470e+07</td>\n",
       "      <td>2.981470e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64240</td>\n",
       "      <td>26847</td>\n",
       "      <td>502</td>\n",
       "      <td>MQTT_Publish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>48579</td>\n",
       "      <td>1883</td>\n",
       "      <td>tcp</td>\n",
       "      <td>mqtt</td>\n",
       "      <td>31.869686</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.282400</td>\n",
       "      <td>0.156889</td>\n",
       "      <td>0.439289</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>296</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>168</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>8.444444</td>\n",
       "      <td>13.115936</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>6.4</td>\n",
       "      <td>9.555103</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>7.714286</td>\n",
       "      <td>11.618477</td>\n",
       "      <td>392.913818</td>\n",
       "      <td>2.982555e+07</td>\n",
       "      <td>3.186969e+07</td>\n",
       "      <td>3.983711e+06</td>\n",
       "      <td>1.045133e+07</td>\n",
       "      <td>7521.152496</td>\n",
       "      <td>1.547557e+06</td>\n",
       "      <td>1.879540e+06</td>\n",
       "      <td>469884.991646</td>\n",
       "      <td>722156.356697</td>\n",
       "      <td>392.913818</td>\n",
       "      <td>2.982555e+07</td>\n",
       "      <td>3.186969e+07</td>\n",
       "      <td>2.451514e+06</td>\n",
       "      <td>8.233270e+06</td>\n",
       "      <td>3.388800</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>25.333333</td>\n",
       "      <td>10.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.044138e+06</td>\n",
       "      <td>2.044138e+06</td>\n",
       "      <td>2.044138e+06</td>\n",
       "      <td>2.044138e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.982555e+07</td>\n",
       "      <td>2.982555e+07</td>\n",
       "      <td>2.982555e+07</td>\n",
       "      <td>2.982555e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64240</td>\n",
       "      <td>26847</td>\n",
       "      <td>502</td>\n",
       "      <td>MQTT_Publish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>54063</td>\n",
       "      <td>1883</td>\n",
       "      <td>tcp</td>\n",
       "      <td>mqtt</td>\n",
       "      <td>32.094711</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.280420</td>\n",
       "      <td>0.155789</td>\n",
       "      <td>0.436209</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>296</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>168</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>8.444444</td>\n",
       "      <td>13.115936</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>6.4</td>\n",
       "      <td>9.555103</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>7.714286</td>\n",
       "      <td>11.618477</td>\n",
       "      <td>385.046005</td>\n",
       "      <td>2.982342e+07</td>\n",
       "      <td>3.209471e+07</td>\n",
       "      <td>4.011839e+06</td>\n",
       "      <td>1.043686e+07</td>\n",
       "      <td>3123.998642</td>\n",
       "      <td>1.503982e+06</td>\n",
       "      <td>2.014587e+06</td>\n",
       "      <td>503646.790981</td>\n",
       "      <td>677274.727026</td>\n",
       "      <td>385.046005</td>\n",
       "      <td>2.982342e+07</td>\n",
       "      <td>3.209471e+07</td>\n",
       "      <td>2.468824e+06</td>\n",
       "      <td>8.226046e+06</td>\n",
       "      <td>3.365040</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>25.333333</td>\n",
       "      <td>10.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.271294e+06</td>\n",
       "      <td>2.271294e+06</td>\n",
       "      <td>2.271294e+06</td>\n",
       "      <td>2.271294e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.982342e+07</td>\n",
       "      <td>2.982342e+07</td>\n",
       "      <td>2.982342e+07</td>\n",
       "      <td>2.982342e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64240</td>\n",
       "      <td>26847</td>\n",
       "      <td>502</td>\n",
       "      <td>MQTT_Publish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>33457</td>\n",
       "      <td>1883</td>\n",
       "      <td>tcp</td>\n",
       "      <td>mqtt</td>\n",
       "      <td>32.104011</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.280339</td>\n",
       "      <td>0.155744</td>\n",
       "      <td>0.436083</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>296</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>168</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>8.444444</td>\n",
       "      <td>13.115936</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>6.4</td>\n",
       "      <td>9.555103</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>7.714286</td>\n",
       "      <td>11.618477</td>\n",
       "      <td>298.023224</td>\n",
       "      <td>2.979543e+07</td>\n",
       "      <td>3.210401e+07</td>\n",
       "      <td>4.013001e+06</td>\n",
       "      <td>1.042510e+07</td>\n",
       "      <td>2321.004868</td>\n",
       "      <td>1.516275e+06</td>\n",
       "      <td>2.045276e+06</td>\n",
       "      <td>511318.981647</td>\n",
       "      <td>681176.138394</td>\n",
       "      <td>275.135040</td>\n",
       "      <td>2.979543e+07</td>\n",
       "      <td>3.210401e+07</td>\n",
       "      <td>2.469539e+06</td>\n",
       "      <td>8.217494e+06</td>\n",
       "      <td>3.364066</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>25.333333</td>\n",
       "      <td>10.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.308578e+06</td>\n",
       "      <td>2.308578e+06</td>\n",
       "      <td>2.308578e+06</td>\n",
       "      <td>2.308578e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.979543e+07</td>\n",
       "      <td>2.979543e+07</td>\n",
       "      <td>2.979543e+07</td>\n",
       "      <td>2.979543e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64240</td>\n",
       "      <td>26847</td>\n",
       "      <td>502</td>\n",
       "      <td>MQTT_Publish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>52181</td>\n",
       "      <td>1883</td>\n",
       "      <td>tcp</td>\n",
       "      <td>mqtt</td>\n",
       "      <td>32.026967</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.281013</td>\n",
       "      <td>0.156118</td>\n",
       "      <td>0.437132</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>296</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>168</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>8.444444</td>\n",
       "      <td>13.115936</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>6.4</td>\n",
       "      <td>9.555103</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>7.714286</td>\n",
       "      <td>11.618477</td>\n",
       "      <td>369.071960</td>\n",
       "      <td>2.966162e+07</td>\n",
       "      <td>3.202697e+07</td>\n",
       "      <td>4.003371e+06</td>\n",
       "      <td>1.037450e+07</td>\n",
       "      <td>2197.980881</td>\n",
       "      <td>1.521419e+06</td>\n",
       "      <td>2.017971e+06</td>\n",
       "      <td>504492.759705</td>\n",
       "      <td>687716.529185</td>\n",
       "      <td>84.161758</td>\n",
       "      <td>2.966162e+07</td>\n",
       "      <td>3.202697e+07</td>\n",
       "      <td>2.463613e+06</td>\n",
       "      <td>8.178452e+06</td>\n",
       "      <td>3.372158</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>25.333333</td>\n",
       "      <td>10.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.365350e+06</td>\n",
       "      <td>2.365350e+06</td>\n",
       "      <td>2.365350e+06</td>\n",
       "      <td>2.365350e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.966162e+07</td>\n",
       "      <td>2.966162e+07</td>\n",
       "      <td>2.966162e+07</td>\n",
       "      <td>2.966162e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64240</td>\n",
       "      <td>26847</td>\n",
       "      <td>502</td>\n",
       "      <td>MQTT_Publish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>53469</td>\n",
       "      <td>1883</td>\n",
       "      <td>tcp</td>\n",
       "      <td>mqtt</td>\n",
       "      <td>32.048637</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.280823</td>\n",
       "      <td>0.156013</td>\n",
       "      <td>0.436836</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>296</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>168</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>8.444444</td>\n",
       "      <td>13.115936</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>6.4</td>\n",
       "      <td>9.555103</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>7.714286</td>\n",
       "      <td>11.618477</td>\n",
       "      <td>379.800797</td>\n",
       "      <td>2.977845e+07</td>\n",
       "      <td>3.204864e+07</td>\n",
       "      <td>4.006080e+06</td>\n",
       "      <td>1.042119e+07</td>\n",
       "      <td>4297.018051</td>\n",
       "      <td>1.512698e+06</td>\n",
       "      <td>2.018293e+06</td>\n",
       "      <td>504573.225975</td>\n",
       "      <td>682043.677238</td>\n",
       "      <td>379.800797</td>\n",
       "      <td>2.977845e+07</td>\n",
       "      <td>3.204864e+07</td>\n",
       "      <td>2.465280e+06</td>\n",
       "      <td>8.213734e+06</td>\n",
       "      <td>3.369878</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>25.333333</td>\n",
       "      <td>10.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.270186e+06</td>\n",
       "      <td>2.270186e+06</td>\n",
       "      <td>2.270186e+06</td>\n",
       "      <td>2.270186e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.977845e+07</td>\n",
       "      <td>2.977845e+07</td>\n",
       "      <td>2.977845e+07</td>\n",
       "      <td>2.977845e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64240</td>\n",
       "      <td>26847</td>\n",
       "      <td>502</td>\n",
       "      <td>MQTT_Publish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  id.orig_p  id.resp_p proto service  flow_duration  \\\n",
       "0           0      38667       1883   tcp    mqtt      32.011598   \n",
       "1           1      51143       1883   tcp    mqtt      31.883584   \n",
       "2           2      44761       1883   tcp    mqtt      32.124053   \n",
       "3           3      60893       1883   tcp    mqtt      31.961063   \n",
       "4           4      51087       1883   tcp    mqtt      31.902362   \n",
       "5           5      48579       1883   tcp    mqtt      31.869686   \n",
       "6           6      54063       1883   tcp    mqtt      32.094711   \n",
       "7           7      33457       1883   tcp    mqtt      32.104011   \n",
       "8           8      52181       1883   tcp    mqtt      32.026967   \n",
       "9           9      53469       1883   tcp    mqtt      32.048637   \n",
       "\n",
       "   fwd_pkts_tot  bwd_pkts_tot  fwd_data_pkts_tot  bwd_data_pkts_tot  \\\n",
       "0             9             5                  3                  3   \n",
       "1             9             5                  3                  3   \n",
       "2             9             5                  3                  3   \n",
       "3             9             5                  3                  3   \n",
       "4             9             5                  3                  3   \n",
       "5             9             5                  3                  3   \n",
       "6             9             5                  3                  3   \n",
       "7             9             5                  3                  3   \n",
       "8             9             5                  3                  3   \n",
       "9             9             5                  3                  3   \n",
       "\n",
       "   fwd_pkts_per_sec  bwd_pkts_per_sec  flow_pkts_per_sec  down_up_ratio  \\\n",
       "0          0.281148          0.156193           0.437341       0.555556   \n",
       "1          0.282277          0.156821           0.439097       0.555556   \n",
       "2          0.280164          0.155647           0.435811       0.555556   \n",
       "3          0.281593          0.156440           0.438033       0.555556   \n",
       "4          0.282111          0.156728           0.438839       0.555556   \n",
       "5          0.282400          0.156889           0.439289       0.555556   \n",
       "6          0.280420          0.155789           0.436209       0.555556   \n",
       "7          0.280339          0.155744           0.436083       0.555556   \n",
       "8          0.281013          0.156118           0.437132       0.555556   \n",
       "9          0.280823          0.156013           0.436836       0.555556   \n",
       "\n",
       "   fwd_header_size_tot  fwd_header_size_min  fwd_header_size_max  \\\n",
       "0                  296                   32                   40   \n",
       "1                  296                   32                   40   \n",
       "2                  296                   32                   40   \n",
       "3                  296                   32                   40   \n",
       "4                  296                   32                   40   \n",
       "5                  296                   32                   40   \n",
       "6                  296                   32                   40   \n",
       "7                  296                   32                   40   \n",
       "8                  296                   32                   40   \n",
       "9                  296                   32                   40   \n",
       "\n",
       "   bwd_header_size_tot  bwd_header_size_min  bwd_header_size_max  \\\n",
       "0                  168                   32                   40   \n",
       "1                  168                   32                   40   \n",
       "2                  168                   32                   40   \n",
       "3                  168                   32                   40   \n",
       "4                  168                   32                   40   \n",
       "5                  168                   32                   40   \n",
       "6                  168                   32                   40   \n",
       "7                  168                   32                   40   \n",
       "8                  168                   32                   40   \n",
       "9                  168                   32                   40   \n",
       "\n",
       "   flow_FIN_flag_count  flow_SYN_flag_count  flow_RST_flag_count  \\\n",
       "0                    0                    2                    1   \n",
       "1                    0                    2                    1   \n",
       "2                    0                    2                    1   \n",
       "3                    0                    2                    1   \n",
       "4                    0                    2                    1   \n",
       "5                    0                    2                    1   \n",
       "6                    0                    2                    1   \n",
       "7                    0                    2                    1   \n",
       "8                    0                    2                    1   \n",
       "9                    0                    2                    1   \n",
       "\n",
       "   fwd_PSH_flag_count  bwd_PSH_flag_count  flow_ACK_flag_count  \\\n",
       "0                   3                   3                   13   \n",
       "1                   3                   3                   13   \n",
       "2                   3                   3                   13   \n",
       "3                   3                   3                   13   \n",
       "4                   3                   3                   13   \n",
       "5                   3                   3                   13   \n",
       "6                   3                   3                   13   \n",
       "7                   3                   3                   13   \n",
       "8                   3                   3                   13   \n",
       "9                   3                   3                   13   \n",
       "\n",
       "   fwd_URG_flag_count  bwd_URG_flag_count  flow_CWR_flag_count  \\\n",
       "0                   0                   0                    0   \n",
       "1                   0                   0                    0   \n",
       "2                   0                   0                    0   \n",
       "3                   0                   0                    0   \n",
       "4                   0                   0                    0   \n",
       "5                   0                   0                    0   \n",
       "6                   0                   0                    0   \n",
       "7                   0                   0                    0   \n",
       "8                   0                   0                    0   \n",
       "9                   0                   0                    0   \n",
       "\n",
       "   flow_ECE_flag_count  fwd_pkts_payload.min  fwd_pkts_payload.max  \\\n",
       "0                    0                   0.0                  33.0   \n",
       "1                    0                   0.0                  33.0   \n",
       "2                    0                   0.0                  33.0   \n",
       "3                    0                   0.0                  33.0   \n",
       "4                    0                   0.0                  33.0   \n",
       "5                    0                   0.0                  33.0   \n",
       "6                    0                   0.0                  33.0   \n",
       "7                    0                   0.0                  33.0   \n",
       "8                    0                   0.0                  33.0   \n",
       "9                    0                   0.0                  33.0   \n",
       "\n",
       "   fwd_pkts_payload.tot  fwd_pkts_payload.avg  fwd_pkts_payload.std  \\\n",
       "0                  76.0              8.444444             13.115936   \n",
       "1                  76.0              8.444444             13.115936   \n",
       "2                  74.0              8.222222             12.852799   \n",
       "3                  74.0              8.222222             12.852799   \n",
       "4                  76.0              8.444444             13.115936   \n",
       "5                  76.0              8.444444             13.115936   \n",
       "6                  76.0              8.444444             13.115936   \n",
       "7                  76.0              8.444444             13.115936   \n",
       "8                  76.0              8.444444             13.115936   \n",
       "9                  76.0              8.444444             13.115936   \n",
       "\n",
       "   bwd_pkts_payload.min  bwd_pkts_payload.max  bwd_pkts_payload.tot  \\\n",
       "0                   0.0                  23.0                  32.0   \n",
       "1                   0.0                  23.0                  32.0   \n",
       "2                   0.0                  21.0                  30.0   \n",
       "3                   0.0                  21.0                  30.0   \n",
       "4                   0.0                  23.0                  32.0   \n",
       "5                   0.0                  23.0                  32.0   \n",
       "6                   0.0                  23.0                  32.0   \n",
       "7                   0.0                  23.0                  32.0   \n",
       "8                   0.0                  23.0                  32.0   \n",
       "9                   0.0                  23.0                  32.0   \n",
       "\n",
       "   bwd_pkts_payload.avg  bwd_pkts_payload.std  flow_pkts_payload.min  \\\n",
       "0                   6.4              9.555103                    0.0   \n",
       "1                   6.4              9.555103                    0.0   \n",
       "2                   6.0              8.689074                    0.0   \n",
       "3                   6.0              8.689074                    0.0   \n",
       "4                   6.4              9.555103                    0.0   \n",
       "5                   6.4              9.555103                    0.0   \n",
       "6                   6.4              9.555103                    0.0   \n",
       "7                   6.4              9.555103                    0.0   \n",
       "8                   6.4              9.555103                    0.0   \n",
       "9                   6.4              9.555103                    0.0   \n",
       "\n",
       "   flow_pkts_payload.max  flow_pkts_payload.tot  flow_pkts_payload.avg  \\\n",
       "0                   33.0                  108.0               7.714286   \n",
       "1                   33.0                  108.0               7.714286   \n",
       "2                   33.0                  104.0               7.428571   \n",
       "3                   33.0                  104.0               7.428571   \n",
       "4                   33.0                  108.0               7.714286   \n",
       "5                   33.0                  108.0               7.714286   \n",
       "6                   33.0                  108.0               7.714286   \n",
       "7                   33.0                  108.0               7.714286   \n",
       "8                   33.0                  108.0               7.714286   \n",
       "9                   33.0                  108.0               7.714286   \n",
       "\n",
       "   flow_pkts_payload.std  fwd_iat.min   fwd_iat.max   fwd_iat.tot  \\\n",
       "0              11.618477   761.985779  2.972918e+07  3.201160e+07   \n",
       "1              11.618477   247.001648  2.985528e+07  3.188358e+07   \n",
       "2              11.229866   283.956528  2.984215e+07  3.212405e+07   \n",
       "3              11.229866   288.963318  2.991377e+07  3.196106e+07   \n",
       "4              11.618477   387.907028  2.981470e+07  3.190236e+07   \n",
       "5              11.618477   392.913818  2.982555e+07  3.186969e+07   \n",
       "6              11.618477   385.046005  2.982342e+07  3.209471e+07   \n",
       "7              11.618477   298.023224  2.979543e+07  3.210401e+07   \n",
       "8              11.618477   369.071960  2.966162e+07  3.202697e+07   \n",
       "9              11.618477   379.800797  2.977845e+07  3.204864e+07   \n",
       "\n",
       "    fwd_iat.avg   fwd_iat.std  bwd_iat.min   bwd_iat.max   bwd_iat.tot  \\\n",
       "0  4.001450e+06  1.040307e+07  4438.877106  1.511694e+06  2.026391e+06   \n",
       "1  3.985448e+06  1.046346e+07  4214.048386  1.576436e+06  1.876261e+06   \n",
       "2  4.015507e+06  1.044238e+07  2456.903458  1.476049e+06  2.013770e+06   \n",
       "3  3.995133e+06  1.048253e+07  3933.906555  1.551892e+06  1.883784e+06   \n",
       "4  3.987795e+06  1.044702e+07  3005.027771  1.632083e+06  1.935984e+06   \n",
       "5  3.983711e+06  1.045133e+07  7521.152496  1.547557e+06  1.879540e+06   \n",
       "6  4.011839e+06  1.043686e+07  3123.998642  1.503982e+06  2.014587e+06   \n",
       "7  4.013001e+06  1.042510e+07  2321.004868  1.516275e+06  2.045276e+06   \n",
       "8  4.003371e+06  1.037450e+07  2197.980881  1.521419e+06  2.017971e+06   \n",
       "9  4.006080e+06  1.042119e+07  4297.018051  1.512698e+06  2.018293e+06   \n",
       "\n",
       "     bwd_iat.avg    bwd_iat.std  flow_iat.min  flow_iat.max  flow_iat.tot  \\\n",
       "0  506597.757339  680406.147126    761.985779  2.972918e+07  3.201160e+07   \n",
       "1  469065.248966  741351.686212    247.001648  2.985528e+07  3.188358e+07   \n",
       "2  503442.466259  660344.360027    283.956528  2.984215e+07  3.212405e+07   \n",
       "3  470946.013927  724569.317911    288.963318  2.991377e+07  3.196106e+07   \n",
       "4  483996.033669  768543.390313    387.907028  2.981470e+07  3.190236e+07   \n",
       "5  469884.991646  722156.356697    392.913818  2.982555e+07  3.186969e+07   \n",
       "6  503646.790981  677274.727026    385.046005  2.982342e+07  3.209471e+07   \n",
       "7  511318.981647  681176.138394    275.135040  2.979543e+07  3.210401e+07   \n",
       "8  504492.759705  687716.529185     84.161758  2.966162e+07  3.202697e+07   \n",
       "9  504573.225975  682043.677238    379.800797  2.977845e+07  3.204864e+07   \n",
       "\n",
       "   flow_iat.avg  flow_iat.std  payload_bytes_per_second  fwd_subflow_pkts  \\\n",
       "0  2.462431e+06  8.199747e+06                  3.373777               3.0   \n",
       "1  2.452583e+06  8.242459e+06                  3.387323               3.0   \n",
       "2  2.471081e+06  8.230593e+06                  3.237450               3.0   \n",
       "3  2.458543e+06  8.257786e+06                  3.253959               3.0   \n",
       "4  2.454028e+06  8.230584e+06                  3.385329               3.0   \n",
       "5  2.451514e+06  8.233270e+06                  3.388800               3.0   \n",
       "6  2.468824e+06  8.226046e+06                  3.365040               3.0   \n",
       "7  2.469539e+06  8.217494e+06                  3.364066               3.0   \n",
       "8  2.463613e+06  8.178452e+06                  3.372158               3.0   \n",
       "9  2.465280e+06  8.213734e+06                  3.369878               3.0   \n",
       "\n",
       "   bwd_subflow_pkts  fwd_subflow_bytes  bwd_subflow_bytes  fwd_bulk_bytes  \\\n",
       "0          1.666667          25.333333          10.666667             0.0   \n",
       "1          1.666667          25.333333          10.666667             0.0   \n",
       "2          1.666667          24.666667          10.000000             0.0   \n",
       "3          1.666667          24.666667          10.000000             0.0   \n",
       "4          1.666667          25.333333          10.666667             0.0   \n",
       "5          1.666667          25.333333          10.666667             0.0   \n",
       "6          1.666667          25.333333          10.666667             0.0   \n",
       "7          1.666667          25.333333          10.666667             0.0   \n",
       "8          1.666667          25.333333          10.666667             0.0   \n",
       "9          1.666667          25.333333          10.666667             0.0   \n",
       "\n",
       "   bwd_bulk_bytes  fwd_bulk_packets  bwd_bulk_packets  fwd_bulk_rate  \\\n",
       "0             0.0               0.0               0.0            0.0   \n",
       "1             0.0               0.0               0.0            0.0   \n",
       "2             0.0               0.0               0.0            0.0   \n",
       "3             0.0               0.0               0.0            0.0   \n",
       "4             0.0               0.0               0.0            0.0   \n",
       "5             0.0               0.0               0.0            0.0   \n",
       "6             0.0               0.0               0.0            0.0   \n",
       "7             0.0               0.0               0.0            0.0   \n",
       "8             0.0               0.0               0.0            0.0   \n",
       "9             0.0               0.0               0.0            0.0   \n",
       "\n",
       "   bwd_bulk_rate    active.min    active.max    active.tot    active.avg  \\\n",
       "0            0.0  2.282415e+06  2.282415e+06  2.282415e+06  2.282415e+06   \n",
       "1            0.0  2.028307e+06  2.028307e+06  2.028307e+06  2.028307e+06   \n",
       "2            0.0  2.281904e+06  2.281904e+06  2.281904e+06  2.281904e+06   \n",
       "3            0.0  2.047288e+06  2.047288e+06  2.047288e+06  2.047288e+06   \n",
       "4            0.0  2.087657e+06  2.087657e+06  2.087657e+06  2.087657e+06   \n",
       "5            0.0  2.044138e+06  2.044138e+06  2.044138e+06  2.044138e+06   \n",
       "6            0.0  2.271294e+06  2.271294e+06  2.271294e+06  2.271294e+06   \n",
       "7            0.0  2.308578e+06  2.308578e+06  2.308578e+06  2.308578e+06   \n",
       "8            0.0  2.365350e+06  2.365350e+06  2.365350e+06  2.365350e+06   \n",
       "9            0.0  2.270186e+06  2.270186e+06  2.270186e+06  2.270186e+06   \n",
       "\n",
       "   active.std      idle.min      idle.max      idle.tot      idle.avg  \\\n",
       "0         0.0  2.972918e+07  2.972918e+07  2.972918e+07  2.972918e+07   \n",
       "1         0.0  2.985528e+07  2.985528e+07  2.985528e+07  2.985528e+07   \n",
       "2         0.0  2.984215e+07  2.984215e+07  2.984215e+07  2.984215e+07   \n",
       "3         0.0  2.991377e+07  2.991377e+07  2.991377e+07  2.991377e+07   \n",
       "4         0.0  2.981470e+07  2.981470e+07  2.981470e+07  2.981470e+07   \n",
       "5         0.0  2.982555e+07  2.982555e+07  2.982555e+07  2.982555e+07   \n",
       "6         0.0  2.982342e+07  2.982342e+07  2.982342e+07  2.982342e+07   \n",
       "7         0.0  2.979543e+07  2.979543e+07  2.979543e+07  2.979543e+07   \n",
       "8         0.0  2.966162e+07  2.966162e+07  2.966162e+07  2.966162e+07   \n",
       "9         0.0  2.977845e+07  2.977845e+07  2.977845e+07  2.977845e+07   \n",
       "\n",
       "   idle.std  fwd_init_window_size  bwd_init_window_size  fwd_last_window_size  \\\n",
       "0       0.0                 64240                 26847                   502   \n",
       "1       0.0                 64240                 26847                   502   \n",
       "2       0.0                 64240                 26847                   502   \n",
       "3       0.0                 64240                 26847                   502   \n",
       "4       0.0                 64240                 26847                   502   \n",
       "5       0.0                 64240                 26847                   502   \n",
       "6       0.0                 64240                 26847                   502   \n",
       "7       0.0                 64240                 26847                   502   \n",
       "8       0.0                 64240                 26847                   502   \n",
       "9       0.0                 64240                 26847                   502   \n",
       "\n",
       "    Attack_type  \n",
       "0  MQTT_Publish  \n",
       "1  MQTT_Publish  \n",
       "2  MQTT_Publish  \n",
       "3  MQTT_Publish  \n",
       "4  MQTT_Publish  \n",
       "5  MQTT_Publish  \n",
       "6  MQTT_Publish  \n",
       "7  MQTT_Publish  \n",
       "8  MQTT_Publish  \n",
       "9  MQTT_Publish  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# 데이터 로드\n",
    "df = pd.read_csv('RT_IoT2022.csv')\n",
    "pd.set_option('display.max_columns', 100) # 뒤에 보려고...\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 123117 entries, 0 to 123116\n",
      "Data columns (total 85 columns):\n",
      " #   Column                    Non-Null Count   Dtype  \n",
      "---  ------                    --------------   -----  \n",
      " 0   Unnamed: 0                123117 non-null  int64  \n",
      " 1   id.orig_p                 123117 non-null  int64  \n",
      " 2   id.resp_p                 123117 non-null  int64  \n",
      " 3   proto                     123117 non-null  object \n",
      " 4   service                   123117 non-null  object \n",
      " 5   flow_duration             123117 non-null  float64\n",
      " 6   fwd_pkts_tot              123117 non-null  int64  \n",
      " 7   bwd_pkts_tot              123117 non-null  int64  \n",
      " 8   fwd_data_pkts_tot         123117 non-null  int64  \n",
      " 9   bwd_data_pkts_tot         123117 non-null  int64  \n",
      " 10  fwd_pkts_per_sec          123117 non-null  float64\n",
      " 11  bwd_pkts_per_sec          123117 non-null  float64\n",
      " 12  flow_pkts_per_sec         123117 non-null  float64\n",
      " 13  down_up_ratio             123117 non-null  float64\n",
      " 14  fwd_header_size_tot       123117 non-null  int64  \n",
      " 15  fwd_header_size_min       123117 non-null  int64  \n",
      " 16  fwd_header_size_max       123117 non-null  int64  \n",
      " 17  bwd_header_size_tot       123117 non-null  int64  \n",
      " 18  bwd_header_size_min       123117 non-null  int64  \n",
      " 19  bwd_header_size_max       123117 non-null  int64  \n",
      " 20  flow_FIN_flag_count       123117 non-null  int64  \n",
      " 21  flow_SYN_flag_count       123117 non-null  int64  \n",
      " 22  flow_RST_flag_count       123117 non-null  int64  \n",
      " 23  fwd_PSH_flag_count        123117 non-null  int64  \n",
      " 24  bwd_PSH_flag_count        123117 non-null  int64  \n",
      " 25  flow_ACK_flag_count       123117 non-null  int64  \n",
      " 26  fwd_URG_flag_count        123117 non-null  int64  \n",
      " 27  bwd_URG_flag_count        123117 non-null  int64  \n",
      " 28  flow_CWR_flag_count       123117 non-null  int64  \n",
      " 29  flow_ECE_flag_count       123117 non-null  int64  \n",
      " 30  fwd_pkts_payload.min      123117 non-null  float64\n",
      " 31  fwd_pkts_payload.max      123117 non-null  float64\n",
      " 32  fwd_pkts_payload.tot      123117 non-null  float64\n",
      " 33  fwd_pkts_payload.avg      123117 non-null  float64\n",
      " 34  fwd_pkts_payload.std      123117 non-null  float64\n",
      " 35  bwd_pkts_payload.min      123117 non-null  float64\n",
      " 36  bwd_pkts_payload.max      123117 non-null  float64\n",
      " 37  bwd_pkts_payload.tot      123117 non-null  float64\n",
      " 38  bwd_pkts_payload.avg      123117 non-null  float64\n",
      " 39  bwd_pkts_payload.std      123117 non-null  float64\n",
      " 40  flow_pkts_payload.min     123117 non-null  float64\n",
      " 41  flow_pkts_payload.max     123117 non-null  float64\n",
      " 42  flow_pkts_payload.tot     123117 non-null  float64\n",
      " 43  flow_pkts_payload.avg     123117 non-null  float64\n",
      " 44  flow_pkts_payload.std     123117 non-null  float64\n",
      " 45  fwd_iat.min               123117 non-null  float64\n",
      " 46  fwd_iat.max               123117 non-null  float64\n",
      " 47  fwd_iat.tot               123117 non-null  float64\n",
      " 48  fwd_iat.avg               123117 non-null  float64\n",
      " 49  fwd_iat.std               123117 non-null  float64\n",
      " 50  bwd_iat.min               123117 non-null  float64\n",
      " 51  bwd_iat.max               123117 non-null  float64\n",
      " 52  bwd_iat.tot               123117 non-null  float64\n",
      " 53  bwd_iat.avg               123117 non-null  float64\n",
      " 54  bwd_iat.std               123117 non-null  float64\n",
      " 55  flow_iat.min              123117 non-null  float64\n",
      " 56  flow_iat.max              123117 non-null  float64\n",
      " 57  flow_iat.tot              123117 non-null  float64\n",
      " 58  flow_iat.avg              123117 non-null  float64\n",
      " 59  flow_iat.std              123117 non-null  float64\n",
      " 60  payload_bytes_per_second  123117 non-null  float64\n",
      " 61  fwd_subflow_pkts          123117 non-null  float64\n",
      " 62  bwd_subflow_pkts          123117 non-null  float64\n",
      " 63  fwd_subflow_bytes         123117 non-null  float64\n",
      " 64  bwd_subflow_bytes         123117 non-null  float64\n",
      " 65  fwd_bulk_bytes            123117 non-null  float64\n",
      " 66  bwd_bulk_bytes            123117 non-null  float64\n",
      " 67  fwd_bulk_packets          123117 non-null  float64\n",
      " 68  bwd_bulk_packets          123117 non-null  float64\n",
      " 69  fwd_bulk_rate             123117 non-null  float64\n",
      " 70  bwd_bulk_rate             123117 non-null  float64\n",
      " 71  active.min                123117 non-null  float64\n",
      " 72  active.max                123117 non-null  float64\n",
      " 73  active.tot                123117 non-null  float64\n",
      " 74  active.avg                123117 non-null  float64\n",
      " 75  active.std                123117 non-null  float64\n",
      " 76  idle.min                  123117 non-null  float64\n",
      " 77  idle.max                  123117 non-null  float64\n",
      " 78  idle.tot                  123117 non-null  float64\n",
      " 79  idle.avg                  123117 non-null  float64\n",
      " 80  idle.std                  123117 non-null  float64\n",
      " 81  fwd_init_window_size      123117 non-null  int64  \n",
      " 82  bwd_init_window_size      123117 non-null  int64  \n",
      " 83  fwd_last_window_size      123117 non-null  int64  \n",
      " 84  Attack_type               123117 non-null  object \n",
      "dtypes: float64(56), int64(26), object(3)\n",
      "memory usage: 79.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# 데이터 정보 확인\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 'Unnamed: 0' column.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(123117, 84)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 불필요한 Column 제거\n",
    "if 'Unnamed: 0' in df.columns:\n",
    "    df = df.drop('Unnamed: 0', axis=1)\n",
    "    print(\"Removed 'Unnamed: 0' column.\")\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id.orig_p               0\n",
       "id.resp_p               0\n",
       "proto                   0\n",
       "service                 0\n",
       "flow_duration           0\n",
       "                       ..\n",
       "idle.std                0\n",
       "fwd_init_window_size    0\n",
       "bwd_init_window_size    0\n",
       "fwd_last_window_size    0\n",
       "Attack_type             0\n",
       "Length: 84, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결측치 확인 -> 결측치 없음\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id.orig_p</th>\n",
       "      <th>id.resp_p</th>\n",
       "      <th>flow_duration</th>\n",
       "      <th>fwd_pkts_tot</th>\n",
       "      <th>bwd_pkts_tot</th>\n",
       "      <th>fwd_data_pkts_tot</th>\n",
       "      <th>bwd_data_pkts_tot</th>\n",
       "      <th>fwd_pkts_per_sec</th>\n",
       "      <th>bwd_pkts_per_sec</th>\n",
       "      <th>flow_pkts_per_sec</th>\n",
       "      <th>down_up_ratio</th>\n",
       "      <th>fwd_header_size_tot</th>\n",
       "      <th>fwd_header_size_min</th>\n",
       "      <th>fwd_header_size_max</th>\n",
       "      <th>bwd_header_size_tot</th>\n",
       "      <th>bwd_header_size_min</th>\n",
       "      <th>bwd_header_size_max</th>\n",
       "      <th>flow_FIN_flag_count</th>\n",
       "      <th>flow_SYN_flag_count</th>\n",
       "      <th>flow_RST_flag_count</th>\n",
       "      <th>fwd_PSH_flag_count</th>\n",
       "      <th>bwd_PSH_flag_count</th>\n",
       "      <th>flow_ACK_flag_count</th>\n",
       "      <th>fwd_URG_flag_count</th>\n",
       "      <th>bwd_URG_flag_count</th>\n",
       "      <th>flow_CWR_flag_count</th>\n",
       "      <th>flow_ECE_flag_count</th>\n",
       "      <th>fwd_pkts_payload.min</th>\n",
       "      <th>fwd_pkts_payload.max</th>\n",
       "      <th>fwd_pkts_payload.tot</th>\n",
       "      <th>fwd_pkts_payload.avg</th>\n",
       "      <th>fwd_pkts_payload.std</th>\n",
       "      <th>bwd_pkts_payload.min</th>\n",
       "      <th>bwd_pkts_payload.max</th>\n",
       "      <th>bwd_pkts_payload.tot</th>\n",
       "      <th>bwd_pkts_payload.avg</th>\n",
       "      <th>bwd_pkts_payload.std</th>\n",
       "      <th>flow_pkts_payload.min</th>\n",
       "      <th>flow_pkts_payload.max</th>\n",
       "      <th>flow_pkts_payload.tot</th>\n",
       "      <th>flow_pkts_payload.avg</th>\n",
       "      <th>flow_pkts_payload.std</th>\n",
       "      <th>fwd_iat.min</th>\n",
       "      <th>fwd_iat.max</th>\n",
       "      <th>fwd_iat.tot</th>\n",
       "      <th>fwd_iat.avg</th>\n",
       "      <th>fwd_iat.std</th>\n",
       "      <th>bwd_iat.min</th>\n",
       "      <th>bwd_iat.max</th>\n",
       "      <th>bwd_iat.tot</th>\n",
       "      <th>bwd_iat.avg</th>\n",
       "      <th>bwd_iat.std</th>\n",
       "      <th>flow_iat.min</th>\n",
       "      <th>flow_iat.max</th>\n",
       "      <th>flow_iat.tot</th>\n",
       "      <th>flow_iat.avg</th>\n",
       "      <th>flow_iat.std</th>\n",
       "      <th>payload_bytes_per_second</th>\n",
       "      <th>fwd_subflow_pkts</th>\n",
       "      <th>bwd_subflow_pkts</th>\n",
       "      <th>fwd_subflow_bytes</th>\n",
       "      <th>bwd_subflow_bytes</th>\n",
       "      <th>fwd_bulk_bytes</th>\n",
       "      <th>bwd_bulk_bytes</th>\n",
       "      <th>fwd_bulk_packets</th>\n",
       "      <th>bwd_bulk_packets</th>\n",
       "      <th>fwd_bulk_rate</th>\n",
       "      <th>bwd_bulk_rate</th>\n",
       "      <th>active.min</th>\n",
       "      <th>active.max</th>\n",
       "      <th>active.tot</th>\n",
       "      <th>active.avg</th>\n",
       "      <th>active.std</th>\n",
       "      <th>idle.min</th>\n",
       "      <th>idle.max</th>\n",
       "      <th>idle.tot</th>\n",
       "      <th>idle.avg</th>\n",
       "      <th>idle.std</th>\n",
       "      <th>fwd_init_window_size</th>\n",
       "      <th>bwd_init_window_size</th>\n",
       "      <th>fwd_last_window_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>123117.000000</td>\n",
       "      <td>123117.000000</td>\n",
       "      <td>123117.000000</td>\n",
       "      <td>123117.000000</td>\n",
       "      <td>123117.000000</td>\n",
       "      <td>123117.000000</td>\n",
       "      <td>123117.000000</td>\n",
       "      <td>1.231170e+05</td>\n",
       "      <td>1.231170e+05</td>\n",
       "      <td>1.231170e+05</td>\n",
       "      <td>123117.000000</td>\n",
       "      <td>123117.000000</td>\n",
       "      <td>123117.000000</td>\n",
       "      <td>123117.000000</td>\n",
       "      <td>123117.000000</td>\n",
       "      <td>123117.000000</td>\n",
       "      <td>123117.000000</td>\n",
       "      <td>123117.000000</td>\n",
       "      <td>123117.000000</td>\n",
       "      <td>123117.000000</td>\n",
       "      <td>123117.000000</td>\n",
       "      <td>123117.000000</td>\n",
       "      <td>123117.000000</td>\n",
       "      <td>123117.000000</td>\n",
       "      <td>123117.0</td>\n",
       "      <td>123117.000000</td>\n",
       "      <td>123117.000000</td>\n",
       "      <td>123117.000000</td>\n",
       "      <td>123117.000000</td>\n",
       "      <td>123117.000000</td>\n",
       "      <td>123117.000000</td>\n",
       "      <td>123117.000000</td>\n",
       "      <td>123117.000000</td>\n",
       "      <td>123117.000000</td>\n",
       "      <td>1.231170e+05</td>\n",
       "      <td>123117.000000</td>\n",
       "      <td>123117.000000</td>\n",
       "      <td>123117.000000</td>\n",
       "      <td>123117.000000</td>\n",
       "      <td>1.231170e+05</td>\n",
       "      <td>123117.000000</td>\n",
       "      <td>123117.000000</td>\n",
       "      <td>1.231170e+05</td>\n",
       "      <td>1.231170e+05</td>\n",
       "      <td>1.231170e+05</td>\n",
       "      <td>1.231170e+05</td>\n",
       "      <td>1.231170e+05</td>\n",
       "      <td>1.231170e+05</td>\n",
       "      <td>1.231170e+05</td>\n",
       "      <td>1.231170e+05</td>\n",
       "      <td>1.231170e+05</td>\n",
       "      <td>1.231170e+05</td>\n",
       "      <td>1.231170e+05</td>\n",
       "      <td>1.231170e+05</td>\n",
       "      <td>1.231170e+05</td>\n",
       "      <td>1.231170e+05</td>\n",
       "      <td>1.231170e+05</td>\n",
       "      <td>1.231170e+05</td>\n",
       "      <td>123117.000000</td>\n",
       "      <td>123117.000000</td>\n",
       "      <td>123117.000000</td>\n",
       "      <td>1.231170e+05</td>\n",
       "      <td>123117.000000</td>\n",
       "      <td>1.231170e+05</td>\n",
       "      <td>123117.000000</td>\n",
       "      <td>123117.000000</td>\n",
       "      <td>1.231170e+05</td>\n",
       "      <td>1.231170e+05</td>\n",
       "      <td>1.231170e+05</td>\n",
       "      <td>1.231170e+05</td>\n",
       "      <td>1.231170e+05</td>\n",
       "      <td>1.231170e+05</td>\n",
       "      <td>1.231170e+05</td>\n",
       "      <td>1.231170e+05</td>\n",
       "      <td>1.231170e+05</td>\n",
       "      <td>1.231170e+05</td>\n",
       "      <td>1.231170e+05</td>\n",
       "      <td>1.231170e+05</td>\n",
       "      <td>123117.000000</td>\n",
       "      <td>123117.000000</td>\n",
       "      <td>123117.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>34639.258738</td>\n",
       "      <td>1014.305092</td>\n",
       "      <td>3.809566</td>\n",
       "      <td>2.268826</td>\n",
       "      <td>1.909509</td>\n",
       "      <td>1.471218</td>\n",
       "      <td>0.820260</td>\n",
       "      <td>3.518063e+05</td>\n",
       "      <td>3.517620e+05</td>\n",
       "      <td>7.035683e+05</td>\n",
       "      <td>0.854571</td>\n",
       "      <td>53.892379</td>\n",
       "      <td>19.779397</td>\n",
       "      <td>20.646637</td>\n",
       "      <td>46.626900</td>\n",
       "      <td>17.695915</td>\n",
       "      <td>18.432678</td>\n",
       "      <td>0.115605</td>\n",
       "      <td>0.950868</td>\n",
       "      <td>0.796454</td>\n",
       "      <td>0.351332</td>\n",
       "      <td>0.393634</td>\n",
       "      <td>2.677737</td>\n",
       "      <td>0.016293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001007</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>96.256073</td>\n",
       "      <td>120.749133</td>\n",
       "      <td>221.519108</td>\n",
       "      <td>100.523827</td>\n",
       "      <td>8.107768</td>\n",
       "      <td>3.817523</td>\n",
       "      <td>52.405005</td>\n",
       "      <td>5.129981e+02</td>\n",
       "      <td>18.786782</td>\n",
       "      <td>20.553025</td>\n",
       "      <td>13.554440</td>\n",
       "      <td>148.510904</td>\n",
       "      <td>7.345172e+02</td>\n",
       "      <td>65.010194</td>\n",
       "      <td>76.041654</td>\n",
       "      <td>8.843266e+03</td>\n",
       "      <td>1.721566e+06</td>\n",
       "      <td>3.780208e+06</td>\n",
       "      <td>2.373575e+05</td>\n",
       "      <td>5.775574e+05</td>\n",
       "      <td>3.764849e+03</td>\n",
       "      <td>4.077267e+05</td>\n",
       "      <td>1.779893e+06</td>\n",
       "      <td>8.765213e+04</td>\n",
       "      <td>1.474803e+05</td>\n",
       "      <td>4.283079e+03</td>\n",
       "      <td>1.725999e+06</td>\n",
       "      <td>3.810575e+06</td>\n",
       "      <td>1.396545e+05</td>\n",
       "      <td>4.501362e+05</td>\n",
       "      <td>4.105345e+07</td>\n",
       "      <td>1.551715</td>\n",
       "      <td>1.337718</td>\n",
       "      <td>136.479504</td>\n",
       "      <td>2.175178e+02</td>\n",
       "      <td>19.249937</td>\n",
       "      <td>1.552337e+02</td>\n",
       "      <td>0.024144</td>\n",
       "      <td>0.131113</td>\n",
       "      <td>3.835746e+03</td>\n",
       "      <td>4.841460e+04</td>\n",
       "      <td>1.331546e+05</td>\n",
       "      <td>1.785898e+05</td>\n",
       "      <td>2.929308e+05</td>\n",
       "      <td>1.481354e+05</td>\n",
       "      <td>2.353599e+04</td>\n",
       "      <td>1.616655e+06</td>\n",
       "      <td>1.701956e+06</td>\n",
       "      <td>3.517644e+06</td>\n",
       "      <td>1.664985e+06</td>\n",
       "      <td>4.550183e+04</td>\n",
       "      <td>6118.905123</td>\n",
       "      <td>2739.776018</td>\n",
       "      <td>751.647514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>19070.620354</td>\n",
       "      <td>5256.371994</td>\n",
       "      <td>130.005408</td>\n",
       "      <td>22.336565</td>\n",
       "      <td>33.018311</td>\n",
       "      <td>19.635196</td>\n",
       "      <td>32.293948</td>\n",
       "      <td>3.707645e+05</td>\n",
       "      <td>3.708015e+05</td>\n",
       "      <td>7.415634e+05</td>\n",
       "      <td>0.337640</td>\n",
       "      <td>393.027195</td>\n",
       "      <td>5.347869</td>\n",
       "      <td>7.230706</td>\n",
       "      <td>1028.228573</td>\n",
       "      <td>7.997577</td>\n",
       "      <td>9.406320</td>\n",
       "      <td>0.475013</td>\n",
       "      <td>0.474262</td>\n",
       "      <td>0.436998</td>\n",
       "      <td>3.951627</td>\n",
       "      <td>6.007443</td>\n",
       "      <td>41.654868</td>\n",
       "      <td>0.126602</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044689</td>\n",
       "      <td>0.031471</td>\n",
       "      <td>45.274996</td>\n",
       "      <td>121.299738</td>\n",
       "      <td>4820.400607</td>\n",
       "      <td>46.097816</td>\n",
       "      <td>45.041897</td>\n",
       "      <td>19.611200</td>\n",
       "      <td>231.305346</td>\n",
       "      <td>4.244143e+04</td>\n",
       "      <td>83.483058</td>\n",
       "      <td>93.397596</td>\n",
       "      <td>35.472227</td>\n",
       "      <td>218.232399</td>\n",
       "      <td>4.296599e+04</td>\n",
       "      <td>50.412145</td>\n",
       "      <td>74.019903</td>\n",
       "      <td>1.226521e+06</td>\n",
       "      <td>9.229085e+06</td>\n",
       "      <td>1.299675e+08</td>\n",
       "      <td>1.895354e+06</td>\n",
       "      <td>3.204813e+06</td>\n",
       "      <td>2.263147e+05</td>\n",
       "      <td>4.288636e+06</td>\n",
       "      <td>9.121196e+07</td>\n",
       "      <td>1.097456e+06</td>\n",
       "      <td>1.784791e+06</td>\n",
       "      <td>2.544910e+05</td>\n",
       "      <td>9.250249e+06</td>\n",
       "      <td>1.300057e+08</td>\n",
       "      <td>8.748026e+05</td>\n",
       "      <td>2.506220e+06</td>\n",
       "      <td>4.485706e+07</td>\n",
       "      <td>2.716957</td>\n",
       "      <td>6.007413</td>\n",
       "      <td>428.905357</td>\n",
       "      <td>7.580753e+03</td>\n",
       "      <td>1974.617810</td>\n",
       "      <td>1.959295e+04</td>\n",
       "      <td>1.484759</td>\n",
       "      <td>14.546545</td>\n",
       "      <td>3.080334e+05</td>\n",
       "      <td>6.852480e+05</td>\n",
       "      <td>1.042332e+06</td>\n",
       "      <td>3.011784e+06</td>\n",
       "      <td>1.445279e+07</td>\n",
       "      <td>1.613007e+06</td>\n",
       "      <td>1.477935e+06</td>\n",
       "      <td>8.809396e+06</td>\n",
       "      <td>9.252337e+06</td>\n",
       "      <td>1.229508e+08</td>\n",
       "      <td>9.007064e+06</td>\n",
       "      <td>1.091361e+06</td>\n",
       "      <td>18716.313861</td>\n",
       "      <td>10018.848534</td>\n",
       "      <td>6310.183843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>17702.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.454354e+01</td>\n",
       "      <td>7.288927e+01</td>\n",
       "      <td>1.490871e+02</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>1.200000e+02</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>50.219518</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>9.536740e-01</td>\n",
       "      <td>9.536740e-01</td>\n",
       "      <td>9.536740e-01</td>\n",
       "      <td>9.536740e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.580981e+03</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>9.536740e-01</td>\n",
       "      <td>9.536740e-01</td>\n",
       "      <td>9.536740e-01</td>\n",
       "      <td>9.536740e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>64.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>37221.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.467238e+05</td>\n",
       "      <td>2.467238e+05</td>\n",
       "      <td>4.934475e+05</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>1.200000e+02</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>84.852814</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.814697e+00</td>\n",
       "      <td>4.053116e+00</td>\n",
       "      <td>4.053116e+00</td>\n",
       "      <td>4.053116e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.960685e+07</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.053116e+00</td>\n",
       "      <td>4.053116e+00</td>\n",
       "      <td>4.053116e+00</td>\n",
       "      <td>4.053116e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>64.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>50971.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.242880e+05</td>\n",
       "      <td>5.242880e+05</td>\n",
       "      <td>1.048576e+06</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>1.200000e+02</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>84.852814</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.006790e+00</td>\n",
       "      <td>5.006790e+00</td>\n",
       "      <td>5.006790e+00</td>\n",
       "      <td>5.006790e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.592405e+07</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.006790e+00</td>\n",
       "      <td>5.006790e+00</td>\n",
       "      <td>5.006790e+00</td>\n",
       "      <td>5.006790e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>64.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>65535.000000</td>\n",
       "      <td>65389.000000</td>\n",
       "      <td>21728.335578</td>\n",
       "      <td>4345.000000</td>\n",
       "      <td>10112.000000</td>\n",
       "      <td>4345.000000</td>\n",
       "      <td>10105.000000</td>\n",
       "      <td>1.048576e+06</td>\n",
       "      <td>1.048576e+06</td>\n",
       "      <td>2.097152e+06</td>\n",
       "      <td>6.087899</td>\n",
       "      <td>69296.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>323592.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>864.000000</td>\n",
       "      <td>1446.000000</td>\n",
       "      <td>11772.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1097.000000</td>\n",
       "      <td>1420.000000</td>\n",
       "      <td>747340.000000</td>\n",
       "      <td>1319.365439</td>\n",
       "      <td>731.579342</td>\n",
       "      <td>1357.000000</td>\n",
       "      <td>5124.000000</td>\n",
       "      <td>1.361042e+07</td>\n",
       "      <td>1457.052632</td>\n",
       "      <td>1506.011875</td>\n",
       "      <td>1097.000000</td>\n",
       "      <td>5124.000000</td>\n",
       "      <td>1.361058e+07</td>\n",
       "      <td>1156.084685</td>\n",
       "      <td>924.651958</td>\n",
       "      <td>3.002526e+08</td>\n",
       "      <td>3.002526e+08</td>\n",
       "      <td>2.172834e+10</td>\n",
       "      <td>3.002526e+08</td>\n",
       "      <td>2.122965e+08</td>\n",
       "      <td>4.319622e+07</td>\n",
       "      <td>3.000282e+08</td>\n",
       "      <td>1.876117e+10</td>\n",
       "      <td>1.501489e+08</td>\n",
       "      <td>2.119613e+08</td>\n",
       "      <td>4.351004e+07</td>\n",
       "      <td>3.000000e+08</td>\n",
       "      <td>2.172834e+10</td>\n",
       "      <td>7.283576e+07</td>\n",
       "      <td>1.341221e+08</td>\n",
       "      <td>1.258291e+08</td>\n",
       "      <td>276.833333</td>\n",
       "      <td>1685.333333</td>\n",
       "      <td>52067.750000</td>\n",
       "      <td>2.268402e+06</td>\n",
       "      <td>465095.000000</td>\n",
       "      <td>6.805208e+06</td>\n",
       "      <td>343.000000</td>\n",
       "      <td>5052.500000</td>\n",
       "      <td>4.633628e+07</td>\n",
       "      <td>2.830087e+07</td>\n",
       "      <td>3.125080e+08</td>\n",
       "      <td>8.480979e+08</td>\n",
       "      <td>2.945221e+09</td>\n",
       "      <td>4.374931e+08</td>\n",
       "      <td>4.774862e+08</td>\n",
       "      <td>3.000000e+08</td>\n",
       "      <td>3.000000e+08</td>\n",
       "      <td>2.096777e+10</td>\n",
       "      <td>3.000000e+08</td>\n",
       "      <td>1.208029e+08</td>\n",
       "      <td>65535.000000</td>\n",
       "      <td>65535.000000</td>\n",
       "      <td>65535.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id.orig_p      id.resp_p  flow_duration   fwd_pkts_tot  \\\n",
       "count  123117.000000  123117.000000  123117.000000  123117.000000   \n",
       "mean    34639.258738    1014.305092       3.809566       2.268826   \n",
       "std     19070.620354    5256.371994     130.005408      22.336565   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%     17702.000000      21.000000       0.000001       1.000000   \n",
       "50%     37221.000000      21.000000       0.000004       1.000000   \n",
       "75%     50971.000000      21.000000       0.000005       1.000000   \n",
       "max     65535.000000   65389.000000   21728.335578    4345.000000   \n",
       "\n",
       "        bwd_pkts_tot  fwd_data_pkts_tot  bwd_data_pkts_tot  fwd_pkts_per_sec  \\\n",
       "count  123117.000000      123117.000000      123117.000000      1.231170e+05   \n",
       "mean        1.909509           1.471218           0.820260      3.518063e+05   \n",
       "std        33.018311          19.635196          32.293948      3.707645e+05   \n",
       "min         0.000000           0.000000           0.000000      0.000000e+00   \n",
       "25%         1.000000           1.000000           0.000000      7.454354e+01   \n",
       "50%         1.000000           1.000000           0.000000      2.467238e+05   \n",
       "75%         1.000000           1.000000           0.000000      5.242880e+05   \n",
       "max     10112.000000        4345.000000       10105.000000      1.048576e+06   \n",
       "\n",
       "       bwd_pkts_per_sec  flow_pkts_per_sec  down_up_ratio  \\\n",
       "count      1.231170e+05       1.231170e+05  123117.000000   \n",
       "mean       3.517620e+05       7.035683e+05       0.854571   \n",
       "std        3.708015e+05       7.415634e+05       0.337640   \n",
       "min        0.000000e+00       0.000000e+00       0.000000   \n",
       "25%        7.288927e+01       1.490871e+02       1.000000   \n",
       "50%        2.467238e+05       4.934475e+05       1.000000   \n",
       "75%        5.242880e+05       1.048576e+06       1.000000   \n",
       "max        1.048576e+06       2.097152e+06       6.087899   \n",
       "\n",
       "       fwd_header_size_tot  fwd_header_size_min  fwd_header_size_max  \\\n",
       "count        123117.000000        123117.000000        123117.000000   \n",
       "mean             53.892379            19.779397            20.646637   \n",
       "std             393.027195             5.347869             7.230706   \n",
       "min               0.000000             0.000000             0.000000   \n",
       "25%              20.000000            20.000000            20.000000   \n",
       "50%              20.000000            20.000000            20.000000   \n",
       "75%              20.000000            20.000000            20.000000   \n",
       "max           69296.000000            44.000000            52.000000   \n",
       "\n",
       "       bwd_header_size_tot  bwd_header_size_min  bwd_header_size_max  \\\n",
       "count        123117.000000        123117.000000        123117.000000   \n",
       "mean             46.626900            17.695915            18.432678   \n",
       "std            1028.228573             7.997577             9.406320   \n",
       "min               0.000000             0.000000             0.000000   \n",
       "25%              20.000000            20.000000            20.000000   \n",
       "50%              20.000000            20.000000            20.000000   \n",
       "75%              20.000000            20.000000            20.000000   \n",
       "max          323592.000000            40.000000            44.000000   \n",
       "\n",
       "       flow_FIN_flag_count  flow_SYN_flag_count  flow_RST_flag_count  \\\n",
       "count        123117.000000        123117.000000        123117.000000   \n",
       "mean              0.115605             0.950868             0.796454   \n",
       "std               0.475013             0.474262             0.436998   \n",
       "min               0.000000             0.000000             0.000000   \n",
       "25%               0.000000             1.000000             1.000000   \n",
       "50%               0.000000             1.000000             1.000000   \n",
       "75%               0.000000             1.000000             1.000000   \n",
       "max              10.000000             8.000000            10.000000   \n",
       "\n",
       "       fwd_PSH_flag_count  bwd_PSH_flag_count  flow_ACK_flag_count  \\\n",
       "count       123117.000000       123117.000000        123117.000000   \n",
       "mean             0.351332            0.393634             2.677737   \n",
       "std              3.951627            6.007443            41.654868   \n",
       "min              0.000000            0.000000             0.000000   \n",
       "25%              0.000000            0.000000             1.000000   \n",
       "50%              0.000000            0.000000             1.000000   \n",
       "75%              0.000000            0.000000             1.000000   \n",
       "max            864.000000         1446.000000         11772.000000   \n",
       "\n",
       "       fwd_URG_flag_count  bwd_URG_flag_count  flow_CWR_flag_count  \\\n",
       "count       123117.000000            123117.0        123117.000000   \n",
       "mean             0.016293                 0.0             0.001007   \n",
       "std              0.126602                 0.0             0.044689   \n",
       "min              0.000000                 0.0             0.000000   \n",
       "25%              0.000000                 0.0             0.000000   \n",
       "50%              0.000000                 0.0             0.000000   \n",
       "75%              0.000000                 0.0             0.000000   \n",
       "max              1.000000                 0.0             4.000000   \n",
       "\n",
       "       flow_ECE_flag_count  fwd_pkts_payload.min  fwd_pkts_payload.max  \\\n",
       "count        123117.000000         123117.000000         123117.000000   \n",
       "mean              0.000699             96.256073            120.749133   \n",
       "std               0.031471             45.274996            121.299738   \n",
       "min               0.000000              0.000000              0.000000   \n",
       "25%               0.000000            120.000000            120.000000   \n",
       "50%               0.000000            120.000000            120.000000   \n",
       "75%               0.000000            120.000000            120.000000   \n",
       "max               4.000000           1097.000000           1420.000000   \n",
       "\n",
       "       fwd_pkts_payload.tot  fwd_pkts_payload.avg  fwd_pkts_payload.std  \\\n",
       "count         123117.000000         123117.000000         123117.000000   \n",
       "mean             221.519108            100.523827              8.107768   \n",
       "std             4820.400607             46.097816             45.041897   \n",
       "min                0.000000              0.000000              0.000000   \n",
       "25%              120.000000            120.000000              0.000000   \n",
       "50%              120.000000            120.000000              0.000000   \n",
       "75%              120.000000            120.000000              0.000000   \n",
       "max           747340.000000           1319.365439            731.579342   \n",
       "\n",
       "       bwd_pkts_payload.min  bwd_pkts_payload.max  bwd_pkts_payload.tot  \\\n",
       "count         123117.000000         123117.000000          1.231170e+05   \n",
       "mean               3.817523             52.405005          5.129981e+02   \n",
       "std               19.611200            231.305346          4.244143e+04   \n",
       "min                0.000000              0.000000          0.000000e+00   \n",
       "25%                0.000000              0.000000          0.000000e+00   \n",
       "50%                0.000000              0.000000          0.000000e+00   \n",
       "75%                0.000000              0.000000          0.000000e+00   \n",
       "max             1357.000000           5124.000000          1.361042e+07   \n",
       "\n",
       "       bwd_pkts_payload.avg  bwd_pkts_payload.std  flow_pkts_payload.min  \\\n",
       "count         123117.000000         123117.000000          123117.000000   \n",
       "mean              18.786782             20.553025              13.554440   \n",
       "std               83.483058             93.397596              35.472227   \n",
       "min                0.000000              0.000000               0.000000   \n",
       "25%                0.000000              0.000000               0.000000   \n",
       "50%                0.000000              0.000000               0.000000   \n",
       "75%                0.000000              0.000000               0.000000   \n",
       "max             1457.052632           1506.011875            1097.000000   \n",
       "\n",
       "       flow_pkts_payload.max  flow_pkts_payload.tot  flow_pkts_payload.avg  \\\n",
       "count          123117.000000           1.231170e+05          123117.000000   \n",
       "mean              148.510904           7.345172e+02              65.010194   \n",
       "std               218.232399           4.296599e+04              50.412145   \n",
       "min                 0.000000           0.000000e+00               0.000000   \n",
       "25%               120.000000           1.200000e+02              60.000000   \n",
       "50%               120.000000           1.200000e+02              60.000000   \n",
       "75%               120.000000           1.200000e+02              60.000000   \n",
       "max              5124.000000           1.361058e+07            1156.084685   \n",
       "\n",
       "       flow_pkts_payload.std   fwd_iat.min   fwd_iat.max   fwd_iat.tot  \\\n",
       "count          123117.000000  1.231170e+05  1.231170e+05  1.231170e+05   \n",
       "mean               76.041654  8.843266e+03  1.721566e+06  3.780208e+06   \n",
       "std                74.019903  1.226521e+06  9.229085e+06  1.299675e+08   \n",
       "min                 0.000000  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%                50.219518  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "50%                84.852814  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "75%                84.852814  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "max               924.651958  3.002526e+08  3.002526e+08  2.172834e+10   \n",
       "\n",
       "        fwd_iat.avg   fwd_iat.std   bwd_iat.min   bwd_iat.max   bwd_iat.tot  \\\n",
       "count  1.231170e+05  1.231170e+05  1.231170e+05  1.231170e+05  1.231170e+05   \n",
       "mean   2.373575e+05  5.775574e+05  3.764849e+03  4.077267e+05  1.779893e+06   \n",
       "std    1.895354e+06  3.204813e+06  2.263147e+05  4.288636e+06  9.121196e+07   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "50%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "75%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "max    3.002526e+08  2.122965e+08  4.319622e+07  3.000282e+08  1.876117e+10   \n",
       "\n",
       "        bwd_iat.avg   bwd_iat.std  flow_iat.min  flow_iat.max  flow_iat.tot  \\\n",
       "count  1.231170e+05  1.231170e+05  1.231170e+05  1.231170e+05  1.231170e+05   \n",
       "mean   8.765213e+04  1.474803e+05  4.283079e+03  1.725999e+06  3.810575e+06   \n",
       "std    1.097456e+06  1.784791e+06  2.544910e+05  9.250249e+06  1.300057e+08   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%    0.000000e+00  0.000000e+00  9.536740e-01  9.536740e-01  9.536740e-01   \n",
       "50%    0.000000e+00  0.000000e+00  3.814697e+00  4.053116e+00  4.053116e+00   \n",
       "75%    0.000000e+00  0.000000e+00  5.006790e+00  5.006790e+00  5.006790e+00   \n",
       "max    1.501489e+08  2.119613e+08  4.351004e+07  3.000000e+08  2.172834e+10   \n",
       "\n",
       "       flow_iat.avg  flow_iat.std  payload_bytes_per_second  fwd_subflow_pkts  \\\n",
       "count  1.231170e+05  1.231170e+05              1.231170e+05     123117.000000   \n",
       "mean   1.396545e+05  4.501362e+05              4.105345e+07          1.551715   \n",
       "std    8.748026e+05  2.506220e+06              4.485706e+07          2.716957   \n",
       "min    0.000000e+00  0.000000e+00              0.000000e+00          0.000000   \n",
       "25%    9.536740e-01  0.000000e+00              2.580981e+03          1.000000   \n",
       "50%    4.053116e+00  0.000000e+00              2.960685e+07          1.000000   \n",
       "75%    5.006790e+00  0.000000e+00              5.592405e+07          1.000000   \n",
       "max    7.283576e+07  1.341221e+08              1.258291e+08        276.833333   \n",
       "\n",
       "       bwd_subflow_pkts  fwd_subflow_bytes  bwd_subflow_bytes  fwd_bulk_bytes  \\\n",
       "count     123117.000000      123117.000000       1.231170e+05   123117.000000   \n",
       "mean           1.337718         136.479504       2.175178e+02       19.249937   \n",
       "std            6.007413         428.905357       7.580753e+03     1974.617810   \n",
       "min            0.000000           0.000000       0.000000e+00        0.000000   \n",
       "25%            1.000000         120.000000       0.000000e+00        0.000000   \n",
       "50%            1.000000         120.000000       0.000000e+00        0.000000   \n",
       "75%            1.000000         120.000000       0.000000e+00        0.000000   \n",
       "max         1685.333333       52067.750000       2.268402e+06   465095.000000   \n",
       "\n",
       "       bwd_bulk_bytes  fwd_bulk_packets  bwd_bulk_packets  fwd_bulk_rate  \\\n",
       "count    1.231170e+05     123117.000000     123117.000000   1.231170e+05   \n",
       "mean     1.552337e+02          0.024144          0.131113   3.835746e+03   \n",
       "std      1.959295e+04          1.484759         14.546545   3.080334e+05   \n",
       "min      0.000000e+00          0.000000          0.000000   0.000000e+00   \n",
       "25%      0.000000e+00          0.000000          0.000000   0.000000e+00   \n",
       "50%      0.000000e+00          0.000000          0.000000   0.000000e+00   \n",
       "75%      0.000000e+00          0.000000          0.000000   0.000000e+00   \n",
       "max      6.805208e+06        343.000000       5052.500000   4.633628e+07   \n",
       "\n",
       "       bwd_bulk_rate    active.min    active.max    active.tot    active.avg  \\\n",
       "count   1.231170e+05  1.231170e+05  1.231170e+05  1.231170e+05  1.231170e+05   \n",
       "mean    4.841460e+04  1.331546e+05  1.785898e+05  2.929308e+05  1.481354e+05   \n",
       "std     6.852480e+05  1.042332e+06  3.011784e+06  1.445279e+07  1.613007e+06   \n",
       "min     0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%     0.000000e+00  9.536740e-01  9.536740e-01  9.536740e-01  9.536740e-01   \n",
       "50%     0.000000e+00  4.053116e+00  4.053116e+00  4.053116e+00  4.053116e+00   \n",
       "75%     0.000000e+00  5.006790e+00  5.006790e+00  5.006790e+00  5.006790e+00   \n",
       "max     2.830087e+07  3.125080e+08  8.480979e+08  2.945221e+09  4.374931e+08   \n",
       "\n",
       "         active.std      idle.min      idle.max      idle.tot      idle.avg  \\\n",
       "count  1.231170e+05  1.231170e+05  1.231170e+05  1.231170e+05  1.231170e+05   \n",
       "mean   2.353599e+04  1.616655e+06  1.701956e+06  3.517644e+06  1.664985e+06   \n",
       "std    1.477935e+06  8.809396e+06  9.252337e+06  1.229508e+08  9.007064e+06   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "50%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "75%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "max    4.774862e+08  3.000000e+08  3.000000e+08  2.096777e+10  3.000000e+08   \n",
       "\n",
       "           idle.std  fwd_init_window_size  bwd_init_window_size  \\\n",
       "count  1.231170e+05         123117.000000         123117.000000   \n",
       "mean   4.550183e+04           6118.905123           2739.776018   \n",
       "std    1.091361e+06          18716.313861          10018.848534   \n",
       "min    0.000000e+00              0.000000              0.000000   \n",
       "25%    0.000000e+00             64.000000              0.000000   \n",
       "50%    0.000000e+00             64.000000              0.000000   \n",
       "75%    0.000000e+00             64.000000              0.000000   \n",
       "max    1.208029e+08          65535.000000          65535.000000   \n",
       "\n",
       "       fwd_last_window_size  \n",
       "count         123117.000000  \n",
       "mean             751.647514  \n",
       "std             6310.183843  \n",
       "min                0.000000  \n",
       "25%               64.000000  \n",
       "50%               64.000000  \n",
       "75%               64.000000  \n",
       "max            65535.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 통계 정보 확인\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2. target 변수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Attack_type\n",
       "DOS_SYN_Hping                 94659\n",
       "Thing_Speak                    8108\n",
       "ARP_poisioning                 7750\n",
       "MQTT_Publish                   4146\n",
       "NMAP_UDP_SCAN                  2590\n",
       "NMAP_XMAS_TREE_SCAN            2010\n",
       "NMAP_OS_DETECTION              2000\n",
       "NMAP_TCP_scan                  1002\n",
       "DDOS_Slowloris                  534\n",
       "Wipro_bulb                      253\n",
       "Metasploit_Brute_Force_SSH       37\n",
       "NMAP_FIN_SCAN                    28\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target variable ('Attack_type') 분포 확인\n",
    "df['Attack_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Normal\n",
       "1    Normal\n",
       "2    Normal\n",
       "3    Normal\n",
       "4    Normal\n",
       "Name: Attack_type, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정상 트래픽으로 간주할 원본 label 목록 정의\n",
    "normal_labels = ['Thing_Speak', 'MQTT_Publish', 'Wipro_bulb'] # Amazon Alexa is missing in the current dataset\n",
    "df['Attack_type'] = df['Attack_type'].apply(lambda x: 'Normal' if x in normal_labels else x)\n",
    "df['Attack_type'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# target variable ('Attack_type') distribution visualization\n",
    "# TODO: `Normal` 클래스가 적어(약 10%) 나중에 불균형 문제를 해결해야 함!\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(x='Attack_type', data=df)\n",
    "plt.title('공격 유형 분포')\n",
    "plt.xlabel('공격 유형')\n",
    "plt.ylabel('빈도')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3. 숫자형(numerical)/범주형(categorical) Column 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of numeric features: 81 e.g. ['id.orig_p', 'id.resp_p', 'flow_duration']\n",
      "Number of categorical features: 2 e.g. ['proto', 'service']\n",
      "Total number of features: 83\n"
     ]
    }
   ],
   "source": [
    "# Feature 중 숫자형/범주형 컬럼 분리 (Attack_type 제외)\n",
    "numeric_features_org = df.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features_org = df.select_dtypes(include='object').columns.tolist()\n",
    "if 'Attack_type' in categorical_features_org:\n",
    "    categorical_features_org.remove('Attack_type') # target variable 제외\n",
    "\n",
    "print(f'Number of numeric features: {len(numeric_features_org)} e.g. {numeric_features_org[:3]}')\n",
    "print(f'Number of categorical features: {len(categorical_features_org)} e.g. {categorical_features_org[:3]}')\n",
    "print(f'Total number of features: {len(numeric_features_org) + len(categorical_features_org)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4. 범주형 자료 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column명: proto \n",
      "Column값 개수: 3\n",
      "proto\n",
      "tcp     110427\n",
      "udp      12633\n",
      "icmp        57\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Column명: service \n",
      "Column값 개수: 10\n",
      "service\n",
      "-         102861\n",
      "dns         9753\n",
      "mqtt        4132\n",
      "http        3464\n",
      "ssl         2663\n",
      "ntp          121\n",
      "dhcp          50\n",
      "irc           43\n",
      "ssh           28\n",
      "radius         2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for col in categorical_features_org:\n",
    "    print(f\"Column명: {col} \\nColumn값 개수: {len(df[col].value_counts())}\")\n",
    "    print(df[col].value_counts())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# service 의 `-`는 사실상 Null 값과 같음.\n",
    "# null 값이 대다수 (전체 123117 데이터 중 102861개 = 83.55%)\n",
    "# 따라서 해당 칼럼도 제거\n",
    "df.drop(columns=['service'], inplace=True)\n",
    "categorical_features_org.remove('service')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. target 변수 Encoding (이진 분류, 다중 분류)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes for Binary Encoding: ['Attack']\n",
      "Binary_Attack_type_Encoded\n",
      "0    123117\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 이진 분류(Binary Classification)용 label 생성\n",
    "df['Binary_Attack_type'] = df['Attack_type'].apply(lambda x: 'Normal' if x == 'Normal' else 'Attack')\n",
    "\n",
    "# 이진 분류용 인코딩\n",
    "binary_label_encoder = LabelEncoder()\n",
    "df['Binary_Attack_type_Encoded'] = binary_label_encoder.fit_transform(df['Binary_Attack_type'])\n",
    "\n",
    "# 이진 분류 인코딩 결과의 클래스 확인\n",
    "print(\"Classes for Binary Encoding:\", binary_label_encoder.classes_)\n",
    "print(df['Binary_Attack_type_Encoded'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes for Multi-Class Encoding: ['ARP_poisioning' 'DDOS_Slowloris' 'DOS_SYN_Hping'\n",
      " 'Metasploit_Brute_Force_SSH' 'NMAP_FIN_SCAN' 'NMAP_OS_DETECTION'\n",
      " 'NMAP_TCP_scan' 'NMAP_UDP_SCAN' 'NMAP_XMAS_TREE_SCAN' 'Normal']\n",
      "Attack_type_Encoded\n",
      "0     7750\n",
      "1      534\n",
      "2    94659\n",
      "3       37\n",
      "4       28\n",
      "5     2000\n",
      "6     1002\n",
      "7     2590\n",
      "8     2010\n",
      "9    12507\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 다중 클래스 분류용 인코딩\n",
    "multi_class_label_encoder = LabelEncoder()\n",
    "df['Attack_type_Encoded'] = multi_class_label_encoder.fit_transform(df['Attack_type'])\n",
    "\n",
    "# 다중 클래스 인코딩 결과의 클래스 확인\n",
    "print(\"Classes for Multi-Class Encoding:\", multi_class_label_encoder.classes_)\n",
    "print(df['Attack_type_Encoded'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Feauture Data 와 target data 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ColumnTransformer 적용하기 이전 shape: (123117, 82)\n"
     ]
    }
   ],
   "source": [
    "target_columns = ['Attack_type', 'Binary_Attack_type', 'Attack_type_Encoded', 'Binary_Attack_type_Encoded']\n",
    "feature_columns = [col for col in df.columns if col not in target_columns]\n",
    "\n",
    "X = df[feature_columns] # X는 target_columns 제외한 나머지\n",
    "y_binary_encoded = df['Binary_Attack_type_Encoded']\n",
    "y_multi_encoded = df['Attack_type_Encoded']\n",
    "\n",
    "print(\"ColumnTransformer 적용하기 이전 shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. 숫자형/범주형 Feature 전처리 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ColumnTransformer 설정:\n",
    "# - 'num': 숫자형 특징에 StandardScaler 적용\n",
    "# - 'cat': 범주형 특징에 OneHotEncoder 적용\n",
    "# OneHotEncoder 적용한 범주형 column은 (service, proto) StandardScaler에서 제외\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features_org),\n",
    "        # 다중공선성 문제 해결을 위해 drop='first' 적용\n",
    "        # categorical_features_org 중 첫번째 column은 제거됨. 이때 나머지 feature들이 모두 0일때 첫 번째 column인 것임\n",
    "        # e.g. \n",
    "        # service의 세가지 값(tcp, udp, icmp) 중 첫번째 column은 tcp이고 이는 제거됨. \n",
    "        # 하나의 row에서 udp, tcp가 둘다 0일때 그 row는 icmp인 것임\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', drop='first'), categorical_features_org)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. 전처리 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 이진 분류용 훈련/테스트 데이터 분리\n",
    "X_train_binary, X_test_binary, y_train_binary, y_test_binary = train_test_split(X, y_binary_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# 다중 분류용 훈련/테스트 데이터 분리\n",
    "X_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(X, y_multi_encoded, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이진 분류용 전처리 적용 후 훈련 데이터 확인:  (98493, 83)\n",
      "이진 분류용 전처리 적용 후 테스트 데이터 확인:  (24624, 83)\n"
     ]
    }
   ],
   "source": [
    "# 이진 분류용 전처리 적용\n",
    "X_train_binary_processed = preprocessor.fit_transform(X_train_binary)\n",
    "X_test_binary_processed = preprocessor.fit_transform(X_test_binary)\n",
    "\n",
    "# 전처리 적용 후 데이터 shape 확인 \n",
    "# 처음 85 - 3('Unknown: 0', 'Attack_type', 'service') - 1(proto) + 3(proto의 세 가지 종류) - 1(drop='first' 적용) = 83 clolumns\n",
    "print(\"이진 분류용 전처리 적용 후 훈련 데이터 확인: \", X_train_binary_processed.shape)\n",
    "print(\"이진 분류용 전처리 적용 후 테스트 데이터 확인: \", X_test_binary_processed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다중 분류용 전처리 적용 후 훈련 데이터 확인:  (98493, 83)\n",
      "다중 분류용 전처리 적용 후 테스트 데이터 확인:  (24624, 83)\n"
     ]
    }
   ],
   "source": [
    "# 다중 분류용 전처리 적용\n",
    "X_train_multi_processed = preprocessor.fit_transform(X_train_multi)\n",
    "X_test_multi_processed = preprocessor.transform(X_test_multi)\n",
    "\n",
    "# 전처리 적용 후 데이터 shape 확인 \n",
    "print(\"다중 분류용 전처리 적용 후 훈련 데이터 확인: \", X_train_multi_processed.shape)\n",
    "print(\"다중 분류용 전처리 적용 후 테스트 데이터 확인: \", X_test_multi_processed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE 적용 후 X shape: (757620, 83)\n",
      "SMOTE 적용 후 y 분포:\n",
      "Attack_type_Encoded\n",
      "0    75762\n",
      "1    75762\n",
      "2    75762\n",
      "3    75762\n",
      "4    75762\n",
      "5    75762\n",
      "6    75762\n",
      "7    75762\n",
      "8    75762\n",
      "9    75762\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 오버 샘플링 진행- 학습 데이터가 inbalanced 하므로\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# SMOTE 적용\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_multi_resampled, y_train_multi_resampled = smote.fit_resample(X_train_multi_processed, y_train_multi)\n",
    "\n",
    "print(\"SMOTE 적용 후 X shape:\", X_train_multi_resampled.shape)\n",
    "print(\"SMOTE 적용 후 y 분포:\")\n",
    "print(pd.Series(y_train_multi_resampled).value_counts().sort_index())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE 적용 후 X shape: (177080, 83)\n",
      "SMOTE 적용 후 y 분포:\n",
      "Binary_Attack_type_Encoded\n",
      "0    88540\n",
      "1    88540\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_binary_resampled, y_train_binary_resampled = smote.fit_resample(X_train_binary_processed, y_train_binary)\n",
    "\n",
    "print(\"SMOTE 적용 후 X shape:\", X_train_binary_resampled.shape)\n",
    "print(\"SMOTE 적용 후 y 분포:\")\n",
    "print(pd.Series(y_train_binary_resampled).value_counts().sort_index())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num__id.orig_p</th>\n",
       "      <th>num__id.resp_p</th>\n",
       "      <th>num__flow_duration</th>\n",
       "      <th>num__fwd_pkts_tot</th>\n",
       "      <th>num__bwd_pkts_tot</th>\n",
       "      <th>num__fwd_data_pkts_tot</th>\n",
       "      <th>num__bwd_data_pkts_tot</th>\n",
       "      <th>num__fwd_pkts_per_sec</th>\n",
       "      <th>num__bwd_pkts_per_sec</th>\n",
       "      <th>num__flow_pkts_per_sec</th>\n",
       "      <th>num__down_up_ratio</th>\n",
       "      <th>num__fwd_header_size_tot</th>\n",
       "      <th>num__fwd_header_size_min</th>\n",
       "      <th>num__fwd_header_size_max</th>\n",
       "      <th>num__bwd_header_size_tot</th>\n",
       "      <th>num__bwd_header_size_min</th>\n",
       "      <th>num__bwd_header_size_max</th>\n",
       "      <th>num__flow_FIN_flag_count</th>\n",
       "      <th>num__flow_SYN_flag_count</th>\n",
       "      <th>num__flow_RST_flag_count</th>\n",
       "      <th>num__fwd_PSH_flag_count</th>\n",
       "      <th>num__bwd_PSH_flag_count</th>\n",
       "      <th>num__flow_ACK_flag_count</th>\n",
       "      <th>num__fwd_URG_flag_count</th>\n",
       "      <th>num__bwd_URG_flag_count</th>\n",
       "      <th>num__flow_CWR_flag_count</th>\n",
       "      <th>num__flow_ECE_flag_count</th>\n",
       "      <th>num__fwd_pkts_payload.min</th>\n",
       "      <th>num__fwd_pkts_payload.max</th>\n",
       "      <th>num__fwd_pkts_payload.tot</th>\n",
       "      <th>num__fwd_pkts_payload.avg</th>\n",
       "      <th>num__fwd_pkts_payload.std</th>\n",
       "      <th>num__bwd_pkts_payload.min</th>\n",
       "      <th>num__bwd_pkts_payload.max</th>\n",
       "      <th>num__bwd_pkts_payload.tot</th>\n",
       "      <th>num__bwd_pkts_payload.avg</th>\n",
       "      <th>num__bwd_pkts_payload.std</th>\n",
       "      <th>num__flow_pkts_payload.min</th>\n",
       "      <th>num__flow_pkts_payload.max</th>\n",
       "      <th>num__flow_pkts_payload.tot</th>\n",
       "      <th>num__flow_pkts_payload.avg</th>\n",
       "      <th>num__flow_pkts_payload.std</th>\n",
       "      <th>num__fwd_iat.min</th>\n",
       "      <th>num__fwd_iat.max</th>\n",
       "      <th>num__fwd_iat.tot</th>\n",
       "      <th>num__fwd_iat.avg</th>\n",
       "      <th>num__fwd_iat.std</th>\n",
       "      <th>num__bwd_iat.min</th>\n",
       "      <th>num__bwd_iat.max</th>\n",
       "      <th>num__bwd_iat.tot</th>\n",
       "      <th>num__bwd_iat.avg</th>\n",
       "      <th>num__bwd_iat.std</th>\n",
       "      <th>num__flow_iat.min</th>\n",
       "      <th>num__flow_iat.max</th>\n",
       "      <th>num__flow_iat.tot</th>\n",
       "      <th>num__flow_iat.avg</th>\n",
       "      <th>num__flow_iat.std</th>\n",
       "      <th>num__payload_bytes_per_second</th>\n",
       "      <th>num__fwd_subflow_pkts</th>\n",
       "      <th>num__bwd_subflow_pkts</th>\n",
       "      <th>num__fwd_subflow_bytes</th>\n",
       "      <th>num__bwd_subflow_bytes</th>\n",
       "      <th>num__fwd_bulk_bytes</th>\n",
       "      <th>num__bwd_bulk_bytes</th>\n",
       "      <th>num__fwd_bulk_packets</th>\n",
       "      <th>num__bwd_bulk_packets</th>\n",
       "      <th>num__fwd_bulk_rate</th>\n",
       "      <th>num__bwd_bulk_rate</th>\n",
       "      <th>num__active.min</th>\n",
       "      <th>num__active.max</th>\n",
       "      <th>num__active.tot</th>\n",
       "      <th>num__active.avg</th>\n",
       "      <th>num__active.std</th>\n",
       "      <th>num__idle.min</th>\n",
       "      <th>num__idle.max</th>\n",
       "      <th>num__idle.tot</th>\n",
       "      <th>num__idle.avg</th>\n",
       "      <th>num__idle.std</th>\n",
       "      <th>num__fwd_init_window_size</th>\n",
       "      <th>num__bwd_init_window_size</th>\n",
       "      <th>num__fwd_last_window_size</th>\n",
       "      <th>cat__proto_tcp</th>\n",
       "      <th>cat__proto_udp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.460811</td>\n",
       "      <td>-0.189327</td>\n",
       "      <td>-0.028333</td>\n",
       "      <td>-0.052978</td>\n",
       "      <td>-0.025377</td>\n",
       "      <td>-0.022892</td>\n",
       "      <td>-0.023265</td>\n",
       "      <td>-0.285354</td>\n",
       "      <td>-0.285211</td>\n",
       "      <td>-0.285284</td>\n",
       "      <td>0.430173</td>\n",
       "      <td>-0.080370</td>\n",
       "      <td>0.043596</td>\n",
       "      <td>-0.087214</td>\n",
       "      <td>-0.024031</td>\n",
       "      <td>0.288595</td>\n",
       "      <td>0.167426</td>\n",
       "      <td>-0.243232</td>\n",
       "      <td>0.105451</td>\n",
       "      <td>0.466136</td>\n",
       "      <td>-0.082591</td>\n",
       "      <td>-0.061478</td>\n",
       "      <td>-0.037392</td>\n",
       "      <td>-0.12952</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.02293</td>\n",
       "      <td>-0.022502</td>\n",
       "      <td>0.525072</td>\n",
       "      <td>-0.004776</td>\n",
       "      <td>-0.020934</td>\n",
       "      <td>0.424667</td>\n",
       "      <td>-0.179677</td>\n",
       "      <td>-0.193291</td>\n",
       "      <td>-0.226052</td>\n",
       "      <td>-0.010964</td>\n",
       "      <td>-0.225570</td>\n",
       "      <td>-0.219344</td>\n",
       "      <td>-0.382317</td>\n",
       "      <td>-0.129843</td>\n",
       "      <td>-0.013079</td>\n",
       "      <td>-0.098209</td>\n",
       "      <td>0.121374</td>\n",
       "      <td>-0.006746</td>\n",
       "      <td>-0.187666</td>\n",
       "      <td>-0.028171</td>\n",
       "      <td>-0.139649</td>\n",
       "      <td>-0.183810</td>\n",
       "      <td>-0.016119</td>\n",
       "      <td>-0.098717</td>\n",
       "      <td>-0.019397</td>\n",
       "      <td>-0.088242</td>\n",
       "      <td>-0.090747</td>\n",
       "      <td>-0.016711</td>\n",
       "      <td>-0.187684</td>\n",
       "      <td>-0.028340</td>\n",
       "      <td>-0.170172</td>\n",
       "      <td>-0.183772</td>\n",
       "      <td>-0.256986</td>\n",
       "      <td>-0.211362</td>\n",
       "      <td>-0.052427</td>\n",
       "      <td>-0.037626</td>\n",
       "      <td>-0.025741</td>\n",
       "      <td>-0.010555</td>\n",
       "      <td>-0.007448</td>\n",
       "      <td>-0.018276</td>\n",
       "      <td>-0.008414</td>\n",
       "      <td>-0.012801</td>\n",
       "      <td>-0.070148</td>\n",
       "      <td>-0.116763</td>\n",
       "      <td>-0.087043</td>\n",
       "      <td>-0.019538</td>\n",
       "      <td>-0.096155</td>\n",
       "      <td>-0.0316</td>\n",
       "      <td>-0.184193</td>\n",
       "      <td>-0.185007</td>\n",
       "      <td>-0.027699</td>\n",
       "      <td>-0.185764</td>\n",
       "      <td>-0.043433</td>\n",
       "      <td>-0.322095</td>\n",
       "      <td>-0.272335</td>\n",
       "      <td>-0.107900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.745200</td>\n",
       "      <td>-0.189327</td>\n",
       "      <td>-0.028333</td>\n",
       "      <td>-0.052978</td>\n",
       "      <td>-0.025377</td>\n",
       "      <td>-0.022892</td>\n",
       "      <td>-0.023265</td>\n",
       "      <td>-0.411925</td>\n",
       "      <td>-0.411770</td>\n",
       "      <td>-0.411849</td>\n",
       "      <td>0.430173</td>\n",
       "      <td>-0.080370</td>\n",
       "      <td>0.043596</td>\n",
       "      <td>-0.087214</td>\n",
       "      <td>-0.024031</td>\n",
       "      <td>0.288595</td>\n",
       "      <td>0.167426</td>\n",
       "      <td>-0.243232</td>\n",
       "      <td>0.105451</td>\n",
       "      <td>0.466136</td>\n",
       "      <td>-0.082591</td>\n",
       "      <td>-0.061478</td>\n",
       "      <td>-0.037392</td>\n",
       "      <td>-0.12952</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.02293</td>\n",
       "      <td>-0.022502</td>\n",
       "      <td>0.525072</td>\n",
       "      <td>-0.004776</td>\n",
       "      <td>-0.020934</td>\n",
       "      <td>0.424667</td>\n",
       "      <td>-0.179677</td>\n",
       "      <td>-0.193291</td>\n",
       "      <td>-0.226052</td>\n",
       "      <td>-0.010964</td>\n",
       "      <td>-0.225570</td>\n",
       "      <td>-0.219344</td>\n",
       "      <td>-0.382317</td>\n",
       "      <td>-0.129843</td>\n",
       "      <td>-0.013079</td>\n",
       "      <td>-0.098209</td>\n",
       "      <td>0.121374</td>\n",
       "      <td>-0.006746</td>\n",
       "      <td>-0.187666</td>\n",
       "      <td>-0.028171</td>\n",
       "      <td>-0.139649</td>\n",
       "      <td>-0.183810</td>\n",
       "      <td>-0.016119</td>\n",
       "      <td>-0.098717</td>\n",
       "      <td>-0.019397</td>\n",
       "      <td>-0.088242</td>\n",
       "      <td>-0.090747</td>\n",
       "      <td>-0.016707</td>\n",
       "      <td>-0.187684</td>\n",
       "      <td>-0.028340</td>\n",
       "      <td>-0.170170</td>\n",
       "      <td>-0.183772</td>\n",
       "      <td>-0.382525</td>\n",
       "      <td>-0.211362</td>\n",
       "      <td>-0.052427</td>\n",
       "      <td>-0.037626</td>\n",
       "      <td>-0.025741</td>\n",
       "      <td>-0.010555</td>\n",
       "      <td>-0.007448</td>\n",
       "      <td>-0.018276</td>\n",
       "      <td>-0.008414</td>\n",
       "      <td>-0.012801</td>\n",
       "      <td>-0.070148</td>\n",
       "      <td>-0.116762</td>\n",
       "      <td>-0.087043</td>\n",
       "      <td>-0.019538</td>\n",
       "      <td>-0.096154</td>\n",
       "      <td>-0.0316</td>\n",
       "      <td>-0.184193</td>\n",
       "      <td>-0.185007</td>\n",
       "      <td>-0.027699</td>\n",
       "      <td>-0.185764</td>\n",
       "      <td>-0.043433</td>\n",
       "      <td>-0.322095</td>\n",
       "      <td>-0.272335</td>\n",
       "      <td>-0.107900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.479444</td>\n",
       "      <td>-0.189327</td>\n",
       "      <td>-0.028333</td>\n",
       "      <td>-0.052978</td>\n",
       "      <td>-0.025377</td>\n",
       "      <td>-0.022892</td>\n",
       "      <td>-0.023265</td>\n",
       "      <td>-0.411925</td>\n",
       "      <td>-0.411770</td>\n",
       "      <td>-0.411849</td>\n",
       "      <td>0.430173</td>\n",
       "      <td>-0.080370</td>\n",
       "      <td>0.043596</td>\n",
       "      <td>-0.087214</td>\n",
       "      <td>-0.024031</td>\n",
       "      <td>0.288595</td>\n",
       "      <td>0.167426</td>\n",
       "      <td>-0.243232</td>\n",
       "      <td>0.105451</td>\n",
       "      <td>0.466136</td>\n",
       "      <td>-0.082591</td>\n",
       "      <td>-0.061478</td>\n",
       "      <td>-0.037392</td>\n",
       "      <td>-0.12952</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.02293</td>\n",
       "      <td>-0.022502</td>\n",
       "      <td>0.525072</td>\n",
       "      <td>-0.004776</td>\n",
       "      <td>-0.020934</td>\n",
       "      <td>0.424667</td>\n",
       "      <td>-0.179677</td>\n",
       "      <td>-0.193291</td>\n",
       "      <td>-0.226052</td>\n",
       "      <td>-0.010964</td>\n",
       "      <td>-0.225570</td>\n",
       "      <td>-0.219344</td>\n",
       "      <td>-0.382317</td>\n",
       "      <td>-0.129843</td>\n",
       "      <td>-0.013079</td>\n",
       "      <td>-0.098209</td>\n",
       "      <td>0.121374</td>\n",
       "      <td>-0.006746</td>\n",
       "      <td>-0.187666</td>\n",
       "      <td>-0.028171</td>\n",
       "      <td>-0.139649</td>\n",
       "      <td>-0.183810</td>\n",
       "      <td>-0.016119</td>\n",
       "      <td>-0.098717</td>\n",
       "      <td>-0.019397</td>\n",
       "      <td>-0.088242</td>\n",
       "      <td>-0.090747</td>\n",
       "      <td>-0.016707</td>\n",
       "      <td>-0.187684</td>\n",
       "      <td>-0.028340</td>\n",
       "      <td>-0.170170</td>\n",
       "      <td>-0.183772</td>\n",
       "      <td>-0.382525</td>\n",
       "      <td>-0.211362</td>\n",
       "      <td>-0.052427</td>\n",
       "      <td>-0.037626</td>\n",
       "      <td>-0.025741</td>\n",
       "      <td>-0.010555</td>\n",
       "      <td>-0.007448</td>\n",
       "      <td>-0.018276</td>\n",
       "      <td>-0.008414</td>\n",
       "      <td>-0.012801</td>\n",
       "      <td>-0.070148</td>\n",
       "      <td>-0.116762</td>\n",
       "      <td>-0.087043</td>\n",
       "      <td>-0.019538</td>\n",
       "      <td>-0.096154</td>\n",
       "      <td>-0.0316</td>\n",
       "      <td>-0.184193</td>\n",
       "      <td>-0.185007</td>\n",
       "      <td>-0.027699</td>\n",
       "      <td>-0.185764</td>\n",
       "      <td>-0.043433</td>\n",
       "      <td>-0.322095</td>\n",
       "      <td>-0.272335</td>\n",
       "      <td>-0.107900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.126457</td>\n",
       "      <td>-0.189327</td>\n",
       "      <td>-0.028333</td>\n",
       "      <td>-0.052978</td>\n",
       "      <td>-0.053133</td>\n",
       "      <td>-0.022892</td>\n",
       "      <td>-0.023265</td>\n",
       "      <td>-0.949851</td>\n",
       "      <td>-0.949644</td>\n",
       "      <td>-0.949750</td>\n",
       "      <td>-2.535151</td>\n",
       "      <td>-0.080370</td>\n",
       "      <td>0.043596</td>\n",
       "      <td>-0.087214</td>\n",
       "      <td>-0.041513</td>\n",
       "      <td>-2.215285</td>\n",
       "      <td>-1.962161</td>\n",
       "      <td>-0.243232</td>\n",
       "      <td>0.105451</td>\n",
       "      <td>-1.823694</td>\n",
       "      <td>-0.082591</td>\n",
       "      <td>-0.061478</td>\n",
       "      <td>-0.059121</td>\n",
       "      <td>-0.12952</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.02293</td>\n",
       "      <td>-0.022502</td>\n",
       "      <td>0.525072</td>\n",
       "      <td>-0.004776</td>\n",
       "      <td>-0.020934</td>\n",
       "      <td>0.424667</td>\n",
       "      <td>-0.179677</td>\n",
       "      <td>-0.193291</td>\n",
       "      <td>-0.226052</td>\n",
       "      <td>-0.010964</td>\n",
       "      <td>-0.225570</td>\n",
       "      <td>-0.219344</td>\n",
       "      <td>3.008303</td>\n",
       "      <td>-0.129843</td>\n",
       "      <td>-0.013079</td>\n",
       "      <td>1.112089</td>\n",
       "      <td>-1.032825</td>\n",
       "      <td>-0.006746</td>\n",
       "      <td>-0.187666</td>\n",
       "      <td>-0.028171</td>\n",
       "      <td>-0.139649</td>\n",
       "      <td>-0.183810</td>\n",
       "      <td>-0.016119</td>\n",
       "      <td>-0.098717</td>\n",
       "      <td>-0.019397</td>\n",
       "      <td>-0.088242</td>\n",
       "      <td>-0.090747</td>\n",
       "      <td>-0.016728</td>\n",
       "      <td>-0.187684</td>\n",
       "      <td>-0.028340</td>\n",
       "      <td>-0.170177</td>\n",
       "      <td>-0.183772</td>\n",
       "      <td>-0.916062</td>\n",
       "      <td>-0.211362</td>\n",
       "      <td>-0.213743</td>\n",
       "      <td>-0.037626</td>\n",
       "      <td>-0.025741</td>\n",
       "      <td>-0.010555</td>\n",
       "      <td>-0.007448</td>\n",
       "      <td>-0.018276</td>\n",
       "      <td>-0.008414</td>\n",
       "      <td>-0.012801</td>\n",
       "      <td>-0.070148</td>\n",
       "      <td>-0.116766</td>\n",
       "      <td>-0.087045</td>\n",
       "      <td>-0.019538</td>\n",
       "      <td>-0.096158</td>\n",
       "      <td>-0.0316</td>\n",
       "      <td>-0.184193</td>\n",
       "      <td>-0.185007</td>\n",
       "      <td>-0.027699</td>\n",
       "      <td>-0.185764</td>\n",
       "      <td>-0.043433</td>\n",
       "      <td>-0.322095</td>\n",
       "      <td>-0.272335</td>\n",
       "      <td>-0.107900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.292922</td>\n",
       "      <td>0.164247</td>\n",
       "      <td>0.075371</td>\n",
       "      <td>0.314758</td>\n",
       "      <td>0.113402</td>\n",
       "      <td>0.069740</td>\n",
       "      <td>0.090233</td>\n",
       "      <td>-0.949850</td>\n",
       "      <td>-0.949643</td>\n",
       "      <td>-0.949749</td>\n",
       "      <td>-0.755956</td>\n",
       "      <td>0.639129</td>\n",
       "      <td>2.292881</td>\n",
       "      <td>2.685367</td>\n",
       "      <td>0.133310</td>\n",
       "      <td>1.790923</td>\n",
       "      <td>2.297012</td>\n",
       "      <td>-0.243232</td>\n",
       "      <td>2.213042</td>\n",
       "      <td>0.466136</td>\n",
       "      <td>0.604128</td>\n",
       "      <td>0.553850</td>\n",
       "      <td>0.266818</td>\n",
       "      <td>-0.12952</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.02293</td>\n",
       "      <td>-0.022502</td>\n",
       "      <td>-2.132323</td>\n",
       "      <td>-0.727023</td>\n",
       "      <td>-0.029292</td>\n",
       "      <td>-2.020930</td>\n",
       "      <td>0.109453</td>\n",
       "      <td>-0.193291</td>\n",
       "      <td>-0.125939</td>\n",
       "      <td>-0.010191</td>\n",
       "      <td>-0.152663</td>\n",
       "      <td>-0.126731</td>\n",
       "      <td>-0.382317</td>\n",
       "      <td>-0.530772</td>\n",
       "      <td>-0.013206</td>\n",
       "      <td>-1.164785</td>\n",
       "      <td>-0.880229</td>\n",
       "      <td>-0.006492</td>\n",
       "      <td>1.217756</td>\n",
       "      <td>0.075549</td>\n",
       "      <td>0.870439</td>\n",
       "      <td>1.160093</td>\n",
       "      <td>-0.015823</td>\n",
       "      <td>0.277125</td>\n",
       "      <td>0.000835</td>\n",
       "      <td>0.330911</td>\n",
       "      <td>0.312921</td>\n",
       "      <td>-0.016438</td>\n",
       "      <td>1.213877</td>\n",
       "      <td>0.075364</td>\n",
       "      <td>1.081787</td>\n",
       "      <td>1.168391</td>\n",
       "      <td>-0.916062</td>\n",
       "      <td>0.699170</td>\n",
       "      <td>0.108888</td>\n",
       "      <td>-0.263074</td>\n",
       "      <td>-0.024226</td>\n",
       "      <td>-0.010555</td>\n",
       "      <td>-0.007448</td>\n",
       "      <td>-0.018276</td>\n",
       "      <td>-0.008414</td>\n",
       "      <td>-0.012801</td>\n",
       "      <td>-0.070148</td>\n",
       "      <td>1.943266</td>\n",
       "      <td>1.092947</td>\n",
       "      <td>0.127038</td>\n",
       "      <td>1.450015</td>\n",
       "      <td>-0.0316</td>\n",
       "      <td>1.287788</td>\n",
       "      <td>1.216235</td>\n",
       "      <td>0.064961</td>\n",
       "      <td>1.253793</td>\n",
       "      <td>-0.043433</td>\n",
       "      <td>3.119271</td>\n",
       "      <td>2.404803</td>\n",
       "      <td>-0.037653</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num__id.orig_p  num__id.resp_p  num__flow_duration  num__fwd_pkts_tot  \\\n",
       "0       -1.460811       -0.189327           -0.028333          -0.052978   \n",
       "1       -0.745200       -0.189327           -0.028333          -0.052978   \n",
       "2        0.479444       -0.189327           -0.028333          -0.052978   \n",
       "3        1.126457       -0.189327           -0.028333          -0.052978   \n",
       "4        0.292922        0.164247            0.075371           0.314758   \n",
       "\n",
       "   num__bwd_pkts_tot  num__fwd_data_pkts_tot  num__bwd_data_pkts_tot  \\\n",
       "0          -0.025377               -0.022892               -0.023265   \n",
       "1          -0.025377               -0.022892               -0.023265   \n",
       "2          -0.025377               -0.022892               -0.023265   \n",
       "3          -0.053133               -0.022892               -0.023265   \n",
       "4           0.113402                0.069740                0.090233   \n",
       "\n",
       "   num__fwd_pkts_per_sec  num__bwd_pkts_per_sec  num__flow_pkts_per_sec  \\\n",
       "0              -0.285354              -0.285211               -0.285284   \n",
       "1              -0.411925              -0.411770               -0.411849   \n",
       "2              -0.411925              -0.411770               -0.411849   \n",
       "3              -0.949851              -0.949644               -0.949750   \n",
       "4              -0.949850              -0.949643               -0.949749   \n",
       "\n",
       "   num__down_up_ratio  num__fwd_header_size_tot  num__fwd_header_size_min  \\\n",
       "0            0.430173                 -0.080370                  0.043596   \n",
       "1            0.430173                 -0.080370                  0.043596   \n",
       "2            0.430173                 -0.080370                  0.043596   \n",
       "3           -2.535151                 -0.080370                  0.043596   \n",
       "4           -0.755956                  0.639129                  2.292881   \n",
       "\n",
       "   num__fwd_header_size_max  num__bwd_header_size_tot  \\\n",
       "0                 -0.087214                 -0.024031   \n",
       "1                 -0.087214                 -0.024031   \n",
       "2                 -0.087214                 -0.024031   \n",
       "3                 -0.087214                 -0.041513   \n",
       "4                  2.685367                  0.133310   \n",
       "\n",
       "   num__bwd_header_size_min  num__bwd_header_size_max  \\\n",
       "0                  0.288595                  0.167426   \n",
       "1                  0.288595                  0.167426   \n",
       "2                  0.288595                  0.167426   \n",
       "3                 -2.215285                 -1.962161   \n",
       "4                  1.790923                  2.297012   \n",
       "\n",
       "   num__flow_FIN_flag_count  num__flow_SYN_flag_count  \\\n",
       "0                 -0.243232                  0.105451   \n",
       "1                 -0.243232                  0.105451   \n",
       "2                 -0.243232                  0.105451   \n",
       "3                 -0.243232                  0.105451   \n",
       "4                 -0.243232                  2.213042   \n",
       "\n",
       "   num__flow_RST_flag_count  num__fwd_PSH_flag_count  num__bwd_PSH_flag_count  \\\n",
       "0                  0.466136                -0.082591                -0.061478   \n",
       "1                  0.466136                -0.082591                -0.061478   \n",
       "2                  0.466136                -0.082591                -0.061478   \n",
       "3                 -1.823694                -0.082591                -0.061478   \n",
       "4                  0.466136                 0.604128                 0.553850   \n",
       "\n",
       "   num__flow_ACK_flag_count  num__fwd_URG_flag_count  num__bwd_URG_flag_count  \\\n",
       "0                 -0.037392                 -0.12952                      0.0   \n",
       "1                 -0.037392                 -0.12952                      0.0   \n",
       "2                 -0.037392                 -0.12952                      0.0   \n",
       "3                 -0.059121                 -0.12952                      0.0   \n",
       "4                  0.266818                 -0.12952                      0.0   \n",
       "\n",
       "   num__flow_CWR_flag_count  num__flow_ECE_flag_count  \\\n",
       "0                  -0.02293                 -0.022502   \n",
       "1                  -0.02293                 -0.022502   \n",
       "2                  -0.02293                 -0.022502   \n",
       "3                  -0.02293                 -0.022502   \n",
       "4                  -0.02293                 -0.022502   \n",
       "\n",
       "   num__fwd_pkts_payload.min  num__fwd_pkts_payload.max  \\\n",
       "0                   0.525072                  -0.004776   \n",
       "1                   0.525072                  -0.004776   \n",
       "2                   0.525072                  -0.004776   \n",
       "3                   0.525072                  -0.004776   \n",
       "4                  -2.132323                  -0.727023   \n",
       "\n",
       "   num__fwd_pkts_payload.tot  num__fwd_pkts_payload.avg  \\\n",
       "0                  -0.020934                   0.424667   \n",
       "1                  -0.020934                   0.424667   \n",
       "2                  -0.020934                   0.424667   \n",
       "3                  -0.020934                   0.424667   \n",
       "4                  -0.029292                  -2.020930   \n",
       "\n",
       "   num__fwd_pkts_payload.std  num__bwd_pkts_payload.min  \\\n",
       "0                  -0.179677                  -0.193291   \n",
       "1                  -0.179677                  -0.193291   \n",
       "2                  -0.179677                  -0.193291   \n",
       "3                  -0.179677                  -0.193291   \n",
       "4                   0.109453                  -0.193291   \n",
       "\n",
       "   num__bwd_pkts_payload.max  num__bwd_pkts_payload.tot  \\\n",
       "0                  -0.226052                  -0.010964   \n",
       "1                  -0.226052                  -0.010964   \n",
       "2                  -0.226052                  -0.010964   \n",
       "3                  -0.226052                  -0.010964   \n",
       "4                  -0.125939                  -0.010191   \n",
       "\n",
       "   num__bwd_pkts_payload.avg  num__bwd_pkts_payload.std  \\\n",
       "0                  -0.225570                  -0.219344   \n",
       "1                  -0.225570                  -0.219344   \n",
       "2                  -0.225570                  -0.219344   \n",
       "3                  -0.225570                  -0.219344   \n",
       "4                  -0.152663                  -0.126731   \n",
       "\n",
       "   num__flow_pkts_payload.min  num__flow_pkts_payload.max  \\\n",
       "0                   -0.382317                   -0.129843   \n",
       "1                   -0.382317                   -0.129843   \n",
       "2                   -0.382317                   -0.129843   \n",
       "3                    3.008303                   -0.129843   \n",
       "4                   -0.382317                   -0.530772   \n",
       "\n",
       "   num__flow_pkts_payload.tot  num__flow_pkts_payload.avg  \\\n",
       "0                   -0.013079                   -0.098209   \n",
       "1                   -0.013079                   -0.098209   \n",
       "2                   -0.013079                   -0.098209   \n",
       "3                   -0.013079                    1.112089   \n",
       "4                   -0.013206                   -1.164785   \n",
       "\n",
       "   num__flow_pkts_payload.std  num__fwd_iat.min  num__fwd_iat.max  \\\n",
       "0                    0.121374         -0.006746         -0.187666   \n",
       "1                    0.121374         -0.006746         -0.187666   \n",
       "2                    0.121374         -0.006746         -0.187666   \n",
       "3                   -1.032825         -0.006746         -0.187666   \n",
       "4                   -0.880229         -0.006492          1.217756   \n",
       "\n",
       "   num__fwd_iat.tot  num__fwd_iat.avg  num__fwd_iat.std  num__bwd_iat.min  \\\n",
       "0         -0.028171         -0.139649         -0.183810         -0.016119   \n",
       "1         -0.028171         -0.139649         -0.183810         -0.016119   \n",
       "2         -0.028171         -0.139649         -0.183810         -0.016119   \n",
       "3         -0.028171         -0.139649         -0.183810         -0.016119   \n",
       "4          0.075549          0.870439          1.160093         -0.015823   \n",
       "\n",
       "   num__bwd_iat.max  num__bwd_iat.tot  num__bwd_iat.avg  num__bwd_iat.std  \\\n",
       "0         -0.098717         -0.019397         -0.088242         -0.090747   \n",
       "1         -0.098717         -0.019397         -0.088242         -0.090747   \n",
       "2         -0.098717         -0.019397         -0.088242         -0.090747   \n",
       "3         -0.098717         -0.019397         -0.088242         -0.090747   \n",
       "4          0.277125          0.000835          0.330911          0.312921   \n",
       "\n",
       "   num__flow_iat.min  num__flow_iat.max  num__flow_iat.tot  num__flow_iat.avg  \\\n",
       "0          -0.016711          -0.187684          -0.028340          -0.170172   \n",
       "1          -0.016707          -0.187684          -0.028340          -0.170170   \n",
       "2          -0.016707          -0.187684          -0.028340          -0.170170   \n",
       "3          -0.016728          -0.187684          -0.028340          -0.170177   \n",
       "4          -0.016438           1.213877           0.075364           1.081787   \n",
       "\n",
       "   num__flow_iat.std  num__payload_bytes_per_second  num__fwd_subflow_pkts  \\\n",
       "0          -0.183772                      -0.256986              -0.211362   \n",
       "1          -0.183772                      -0.382525              -0.211362   \n",
       "2          -0.183772                      -0.382525              -0.211362   \n",
       "3          -0.183772                      -0.916062              -0.211362   \n",
       "4           1.168391                      -0.916062               0.699170   \n",
       "\n",
       "   num__bwd_subflow_pkts  num__fwd_subflow_bytes  num__bwd_subflow_bytes  \\\n",
       "0              -0.052427               -0.037626               -0.025741   \n",
       "1              -0.052427               -0.037626               -0.025741   \n",
       "2              -0.052427               -0.037626               -0.025741   \n",
       "3              -0.213743               -0.037626               -0.025741   \n",
       "4               0.108888               -0.263074               -0.024226   \n",
       "\n",
       "   num__fwd_bulk_bytes  num__bwd_bulk_bytes  num__fwd_bulk_packets  \\\n",
       "0            -0.010555            -0.007448              -0.018276   \n",
       "1            -0.010555            -0.007448              -0.018276   \n",
       "2            -0.010555            -0.007448              -0.018276   \n",
       "3            -0.010555            -0.007448              -0.018276   \n",
       "4            -0.010555            -0.007448              -0.018276   \n",
       "\n",
       "   num__bwd_bulk_packets  num__fwd_bulk_rate  num__bwd_bulk_rate  \\\n",
       "0              -0.008414           -0.012801           -0.070148   \n",
       "1              -0.008414           -0.012801           -0.070148   \n",
       "2              -0.008414           -0.012801           -0.070148   \n",
       "3              -0.008414           -0.012801           -0.070148   \n",
       "4              -0.008414           -0.012801           -0.070148   \n",
       "\n",
       "   num__active.min  num__active.max  num__active.tot  num__active.avg  \\\n",
       "0        -0.116763        -0.087043        -0.019538        -0.096155   \n",
       "1        -0.116762        -0.087043        -0.019538        -0.096154   \n",
       "2        -0.116762        -0.087043        -0.019538        -0.096154   \n",
       "3        -0.116766        -0.087045        -0.019538        -0.096158   \n",
       "4         1.943266         1.092947         0.127038         1.450015   \n",
       "\n",
       "   num__active.std  num__idle.min  num__idle.max  num__idle.tot  \\\n",
       "0          -0.0316      -0.184193      -0.185007      -0.027699   \n",
       "1          -0.0316      -0.184193      -0.185007      -0.027699   \n",
       "2          -0.0316      -0.184193      -0.185007      -0.027699   \n",
       "3          -0.0316      -0.184193      -0.185007      -0.027699   \n",
       "4          -0.0316       1.287788       1.216235       0.064961   \n",
       "\n",
       "   num__idle.avg  num__idle.std  num__fwd_init_window_size  \\\n",
       "0      -0.185764      -0.043433                  -0.322095   \n",
       "1      -0.185764      -0.043433                  -0.322095   \n",
       "2      -0.185764      -0.043433                  -0.322095   \n",
       "3      -0.185764      -0.043433                  -0.322095   \n",
       "4       1.253793      -0.043433                   3.119271   \n",
       "\n",
       "   num__bwd_init_window_size  num__fwd_last_window_size  cat__proto_tcp  \\\n",
       "0                  -0.272335                  -0.107900             1.0   \n",
       "1                  -0.272335                  -0.107900             1.0   \n",
       "2                  -0.272335                  -0.107900             1.0   \n",
       "3                  -0.272335                  -0.107900             1.0   \n",
       "4                   2.404803                  -0.037653             1.0   \n",
       "\n",
       "   cat__proto_udp  \n",
       "0             0.0  \n",
       "1             0.0  \n",
       "2             0.0  \n",
       "3             0.0  \n",
       "4             0.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이진 분류용 전처리 적용 후 데이터 확인\n",
    "features_names_binary = preprocessor.get_feature_names_out()\n",
    "X_processed_binary = pd.DataFrame(X_train_binary_processed, columns=features_names_binary)\n",
    "X_processed_binary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num__id.orig_p</th>\n",
       "      <th>num__id.resp_p</th>\n",
       "      <th>num__flow_duration</th>\n",
       "      <th>num__fwd_pkts_tot</th>\n",
       "      <th>num__bwd_pkts_tot</th>\n",
       "      <th>num__fwd_data_pkts_tot</th>\n",
       "      <th>num__bwd_data_pkts_tot</th>\n",
       "      <th>num__fwd_pkts_per_sec</th>\n",
       "      <th>num__bwd_pkts_per_sec</th>\n",
       "      <th>num__flow_pkts_per_sec</th>\n",
       "      <th>num__down_up_ratio</th>\n",
       "      <th>num__fwd_header_size_tot</th>\n",
       "      <th>num__fwd_header_size_min</th>\n",
       "      <th>num__fwd_header_size_max</th>\n",
       "      <th>num__bwd_header_size_tot</th>\n",
       "      <th>num__bwd_header_size_min</th>\n",
       "      <th>num__bwd_header_size_max</th>\n",
       "      <th>num__flow_FIN_flag_count</th>\n",
       "      <th>num__flow_SYN_flag_count</th>\n",
       "      <th>num__flow_RST_flag_count</th>\n",
       "      <th>num__fwd_PSH_flag_count</th>\n",
       "      <th>num__bwd_PSH_flag_count</th>\n",
       "      <th>num__flow_ACK_flag_count</th>\n",
       "      <th>num__fwd_URG_flag_count</th>\n",
       "      <th>num__bwd_URG_flag_count</th>\n",
       "      <th>num__flow_CWR_flag_count</th>\n",
       "      <th>num__flow_ECE_flag_count</th>\n",
       "      <th>num__fwd_pkts_payload.min</th>\n",
       "      <th>num__fwd_pkts_payload.max</th>\n",
       "      <th>num__fwd_pkts_payload.tot</th>\n",
       "      <th>num__fwd_pkts_payload.avg</th>\n",
       "      <th>num__fwd_pkts_payload.std</th>\n",
       "      <th>num__bwd_pkts_payload.min</th>\n",
       "      <th>num__bwd_pkts_payload.max</th>\n",
       "      <th>num__bwd_pkts_payload.tot</th>\n",
       "      <th>num__bwd_pkts_payload.avg</th>\n",
       "      <th>num__bwd_pkts_payload.std</th>\n",
       "      <th>num__flow_pkts_payload.min</th>\n",
       "      <th>num__flow_pkts_payload.max</th>\n",
       "      <th>num__flow_pkts_payload.tot</th>\n",
       "      <th>num__flow_pkts_payload.avg</th>\n",
       "      <th>num__flow_pkts_payload.std</th>\n",
       "      <th>num__fwd_iat.min</th>\n",
       "      <th>num__fwd_iat.max</th>\n",
       "      <th>num__fwd_iat.tot</th>\n",
       "      <th>num__fwd_iat.avg</th>\n",
       "      <th>num__fwd_iat.std</th>\n",
       "      <th>num__bwd_iat.min</th>\n",
       "      <th>num__bwd_iat.max</th>\n",
       "      <th>num__bwd_iat.tot</th>\n",
       "      <th>num__bwd_iat.avg</th>\n",
       "      <th>num__bwd_iat.std</th>\n",
       "      <th>num__flow_iat.min</th>\n",
       "      <th>num__flow_iat.max</th>\n",
       "      <th>num__flow_iat.tot</th>\n",
       "      <th>num__flow_iat.avg</th>\n",
       "      <th>num__flow_iat.std</th>\n",
       "      <th>num__payload_bytes_per_second</th>\n",
       "      <th>num__fwd_subflow_pkts</th>\n",
       "      <th>num__bwd_subflow_pkts</th>\n",
       "      <th>num__fwd_subflow_bytes</th>\n",
       "      <th>num__bwd_subflow_bytes</th>\n",
       "      <th>num__fwd_bulk_bytes</th>\n",
       "      <th>num__bwd_bulk_bytes</th>\n",
       "      <th>num__fwd_bulk_packets</th>\n",
       "      <th>num__bwd_bulk_packets</th>\n",
       "      <th>num__fwd_bulk_rate</th>\n",
       "      <th>num__bwd_bulk_rate</th>\n",
       "      <th>num__active.min</th>\n",
       "      <th>num__active.max</th>\n",
       "      <th>num__active.tot</th>\n",
       "      <th>num__active.avg</th>\n",
       "      <th>num__active.std</th>\n",
       "      <th>num__idle.min</th>\n",
       "      <th>num__idle.max</th>\n",
       "      <th>num__idle.tot</th>\n",
       "      <th>num__idle.avg</th>\n",
       "      <th>num__idle.std</th>\n",
       "      <th>num__fwd_init_window_size</th>\n",
       "      <th>num__bwd_init_window_size</th>\n",
       "      <th>num__fwd_last_window_size</th>\n",
       "      <th>cat__proto_tcp</th>\n",
       "      <th>cat__proto_udp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.460811</td>\n",
       "      <td>-0.189327</td>\n",
       "      <td>-0.028333</td>\n",
       "      <td>-0.052978</td>\n",
       "      <td>-0.025377</td>\n",
       "      <td>-0.022892</td>\n",
       "      <td>-0.023265</td>\n",
       "      <td>-0.285354</td>\n",
       "      <td>-0.285211</td>\n",
       "      <td>-0.285284</td>\n",
       "      <td>0.430173</td>\n",
       "      <td>-0.080370</td>\n",
       "      <td>0.043596</td>\n",
       "      <td>-0.087214</td>\n",
       "      <td>-0.024031</td>\n",
       "      <td>0.288595</td>\n",
       "      <td>0.167426</td>\n",
       "      <td>-0.243232</td>\n",
       "      <td>0.105451</td>\n",
       "      <td>0.466136</td>\n",
       "      <td>-0.082591</td>\n",
       "      <td>-0.061478</td>\n",
       "      <td>-0.037392</td>\n",
       "      <td>-0.12952</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.02293</td>\n",
       "      <td>-0.022502</td>\n",
       "      <td>0.525072</td>\n",
       "      <td>-0.004776</td>\n",
       "      <td>-0.020934</td>\n",
       "      <td>0.424667</td>\n",
       "      <td>-0.179677</td>\n",
       "      <td>-0.193291</td>\n",
       "      <td>-0.226052</td>\n",
       "      <td>-0.010964</td>\n",
       "      <td>-0.225570</td>\n",
       "      <td>-0.219344</td>\n",
       "      <td>-0.382317</td>\n",
       "      <td>-0.129843</td>\n",
       "      <td>-0.013079</td>\n",
       "      <td>-0.098209</td>\n",
       "      <td>0.121374</td>\n",
       "      <td>-0.006746</td>\n",
       "      <td>-0.187666</td>\n",
       "      <td>-0.028171</td>\n",
       "      <td>-0.139649</td>\n",
       "      <td>-0.183810</td>\n",
       "      <td>-0.016119</td>\n",
       "      <td>-0.098717</td>\n",
       "      <td>-0.019397</td>\n",
       "      <td>-0.088242</td>\n",
       "      <td>-0.090747</td>\n",
       "      <td>-0.016711</td>\n",
       "      <td>-0.187684</td>\n",
       "      <td>-0.028340</td>\n",
       "      <td>-0.170172</td>\n",
       "      <td>-0.183772</td>\n",
       "      <td>-0.256986</td>\n",
       "      <td>-0.211362</td>\n",
       "      <td>-0.052427</td>\n",
       "      <td>-0.037626</td>\n",
       "      <td>-0.025741</td>\n",
       "      <td>-0.010555</td>\n",
       "      <td>-0.007448</td>\n",
       "      <td>-0.018276</td>\n",
       "      <td>-0.008414</td>\n",
       "      <td>-0.012801</td>\n",
       "      <td>-0.070148</td>\n",
       "      <td>-0.116763</td>\n",
       "      <td>-0.087043</td>\n",
       "      <td>-0.019538</td>\n",
       "      <td>-0.096155</td>\n",
       "      <td>-0.0316</td>\n",
       "      <td>-0.184193</td>\n",
       "      <td>-0.185007</td>\n",
       "      <td>-0.027699</td>\n",
       "      <td>-0.185764</td>\n",
       "      <td>-0.043433</td>\n",
       "      <td>-0.322095</td>\n",
       "      <td>-0.272335</td>\n",
       "      <td>-0.107900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.745200</td>\n",
       "      <td>-0.189327</td>\n",
       "      <td>-0.028333</td>\n",
       "      <td>-0.052978</td>\n",
       "      <td>-0.025377</td>\n",
       "      <td>-0.022892</td>\n",
       "      <td>-0.023265</td>\n",
       "      <td>-0.411925</td>\n",
       "      <td>-0.411770</td>\n",
       "      <td>-0.411849</td>\n",
       "      <td>0.430173</td>\n",
       "      <td>-0.080370</td>\n",
       "      <td>0.043596</td>\n",
       "      <td>-0.087214</td>\n",
       "      <td>-0.024031</td>\n",
       "      <td>0.288595</td>\n",
       "      <td>0.167426</td>\n",
       "      <td>-0.243232</td>\n",
       "      <td>0.105451</td>\n",
       "      <td>0.466136</td>\n",
       "      <td>-0.082591</td>\n",
       "      <td>-0.061478</td>\n",
       "      <td>-0.037392</td>\n",
       "      <td>-0.12952</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.02293</td>\n",
       "      <td>-0.022502</td>\n",
       "      <td>0.525072</td>\n",
       "      <td>-0.004776</td>\n",
       "      <td>-0.020934</td>\n",
       "      <td>0.424667</td>\n",
       "      <td>-0.179677</td>\n",
       "      <td>-0.193291</td>\n",
       "      <td>-0.226052</td>\n",
       "      <td>-0.010964</td>\n",
       "      <td>-0.225570</td>\n",
       "      <td>-0.219344</td>\n",
       "      <td>-0.382317</td>\n",
       "      <td>-0.129843</td>\n",
       "      <td>-0.013079</td>\n",
       "      <td>-0.098209</td>\n",
       "      <td>0.121374</td>\n",
       "      <td>-0.006746</td>\n",
       "      <td>-0.187666</td>\n",
       "      <td>-0.028171</td>\n",
       "      <td>-0.139649</td>\n",
       "      <td>-0.183810</td>\n",
       "      <td>-0.016119</td>\n",
       "      <td>-0.098717</td>\n",
       "      <td>-0.019397</td>\n",
       "      <td>-0.088242</td>\n",
       "      <td>-0.090747</td>\n",
       "      <td>-0.016707</td>\n",
       "      <td>-0.187684</td>\n",
       "      <td>-0.028340</td>\n",
       "      <td>-0.170170</td>\n",
       "      <td>-0.183772</td>\n",
       "      <td>-0.382525</td>\n",
       "      <td>-0.211362</td>\n",
       "      <td>-0.052427</td>\n",
       "      <td>-0.037626</td>\n",
       "      <td>-0.025741</td>\n",
       "      <td>-0.010555</td>\n",
       "      <td>-0.007448</td>\n",
       "      <td>-0.018276</td>\n",
       "      <td>-0.008414</td>\n",
       "      <td>-0.012801</td>\n",
       "      <td>-0.070148</td>\n",
       "      <td>-0.116762</td>\n",
       "      <td>-0.087043</td>\n",
       "      <td>-0.019538</td>\n",
       "      <td>-0.096154</td>\n",
       "      <td>-0.0316</td>\n",
       "      <td>-0.184193</td>\n",
       "      <td>-0.185007</td>\n",
       "      <td>-0.027699</td>\n",
       "      <td>-0.185764</td>\n",
       "      <td>-0.043433</td>\n",
       "      <td>-0.322095</td>\n",
       "      <td>-0.272335</td>\n",
       "      <td>-0.107900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.479444</td>\n",
       "      <td>-0.189327</td>\n",
       "      <td>-0.028333</td>\n",
       "      <td>-0.052978</td>\n",
       "      <td>-0.025377</td>\n",
       "      <td>-0.022892</td>\n",
       "      <td>-0.023265</td>\n",
       "      <td>-0.411925</td>\n",
       "      <td>-0.411770</td>\n",
       "      <td>-0.411849</td>\n",
       "      <td>0.430173</td>\n",
       "      <td>-0.080370</td>\n",
       "      <td>0.043596</td>\n",
       "      <td>-0.087214</td>\n",
       "      <td>-0.024031</td>\n",
       "      <td>0.288595</td>\n",
       "      <td>0.167426</td>\n",
       "      <td>-0.243232</td>\n",
       "      <td>0.105451</td>\n",
       "      <td>0.466136</td>\n",
       "      <td>-0.082591</td>\n",
       "      <td>-0.061478</td>\n",
       "      <td>-0.037392</td>\n",
       "      <td>-0.12952</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.02293</td>\n",
       "      <td>-0.022502</td>\n",
       "      <td>0.525072</td>\n",
       "      <td>-0.004776</td>\n",
       "      <td>-0.020934</td>\n",
       "      <td>0.424667</td>\n",
       "      <td>-0.179677</td>\n",
       "      <td>-0.193291</td>\n",
       "      <td>-0.226052</td>\n",
       "      <td>-0.010964</td>\n",
       "      <td>-0.225570</td>\n",
       "      <td>-0.219344</td>\n",
       "      <td>-0.382317</td>\n",
       "      <td>-0.129843</td>\n",
       "      <td>-0.013079</td>\n",
       "      <td>-0.098209</td>\n",
       "      <td>0.121374</td>\n",
       "      <td>-0.006746</td>\n",
       "      <td>-0.187666</td>\n",
       "      <td>-0.028171</td>\n",
       "      <td>-0.139649</td>\n",
       "      <td>-0.183810</td>\n",
       "      <td>-0.016119</td>\n",
       "      <td>-0.098717</td>\n",
       "      <td>-0.019397</td>\n",
       "      <td>-0.088242</td>\n",
       "      <td>-0.090747</td>\n",
       "      <td>-0.016707</td>\n",
       "      <td>-0.187684</td>\n",
       "      <td>-0.028340</td>\n",
       "      <td>-0.170170</td>\n",
       "      <td>-0.183772</td>\n",
       "      <td>-0.382525</td>\n",
       "      <td>-0.211362</td>\n",
       "      <td>-0.052427</td>\n",
       "      <td>-0.037626</td>\n",
       "      <td>-0.025741</td>\n",
       "      <td>-0.010555</td>\n",
       "      <td>-0.007448</td>\n",
       "      <td>-0.018276</td>\n",
       "      <td>-0.008414</td>\n",
       "      <td>-0.012801</td>\n",
       "      <td>-0.070148</td>\n",
       "      <td>-0.116762</td>\n",
       "      <td>-0.087043</td>\n",
       "      <td>-0.019538</td>\n",
       "      <td>-0.096154</td>\n",
       "      <td>-0.0316</td>\n",
       "      <td>-0.184193</td>\n",
       "      <td>-0.185007</td>\n",
       "      <td>-0.027699</td>\n",
       "      <td>-0.185764</td>\n",
       "      <td>-0.043433</td>\n",
       "      <td>-0.322095</td>\n",
       "      <td>-0.272335</td>\n",
       "      <td>-0.107900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.126457</td>\n",
       "      <td>-0.189327</td>\n",
       "      <td>-0.028333</td>\n",
       "      <td>-0.052978</td>\n",
       "      <td>-0.053133</td>\n",
       "      <td>-0.022892</td>\n",
       "      <td>-0.023265</td>\n",
       "      <td>-0.949851</td>\n",
       "      <td>-0.949644</td>\n",
       "      <td>-0.949750</td>\n",
       "      <td>-2.535151</td>\n",
       "      <td>-0.080370</td>\n",
       "      <td>0.043596</td>\n",
       "      <td>-0.087214</td>\n",
       "      <td>-0.041513</td>\n",
       "      <td>-2.215285</td>\n",
       "      <td>-1.962161</td>\n",
       "      <td>-0.243232</td>\n",
       "      <td>0.105451</td>\n",
       "      <td>-1.823694</td>\n",
       "      <td>-0.082591</td>\n",
       "      <td>-0.061478</td>\n",
       "      <td>-0.059121</td>\n",
       "      <td>-0.12952</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.02293</td>\n",
       "      <td>-0.022502</td>\n",
       "      <td>0.525072</td>\n",
       "      <td>-0.004776</td>\n",
       "      <td>-0.020934</td>\n",
       "      <td>0.424667</td>\n",
       "      <td>-0.179677</td>\n",
       "      <td>-0.193291</td>\n",
       "      <td>-0.226052</td>\n",
       "      <td>-0.010964</td>\n",
       "      <td>-0.225570</td>\n",
       "      <td>-0.219344</td>\n",
       "      <td>3.008303</td>\n",
       "      <td>-0.129843</td>\n",
       "      <td>-0.013079</td>\n",
       "      <td>1.112089</td>\n",
       "      <td>-1.032825</td>\n",
       "      <td>-0.006746</td>\n",
       "      <td>-0.187666</td>\n",
       "      <td>-0.028171</td>\n",
       "      <td>-0.139649</td>\n",
       "      <td>-0.183810</td>\n",
       "      <td>-0.016119</td>\n",
       "      <td>-0.098717</td>\n",
       "      <td>-0.019397</td>\n",
       "      <td>-0.088242</td>\n",
       "      <td>-0.090747</td>\n",
       "      <td>-0.016728</td>\n",
       "      <td>-0.187684</td>\n",
       "      <td>-0.028340</td>\n",
       "      <td>-0.170177</td>\n",
       "      <td>-0.183772</td>\n",
       "      <td>-0.916062</td>\n",
       "      <td>-0.211362</td>\n",
       "      <td>-0.213743</td>\n",
       "      <td>-0.037626</td>\n",
       "      <td>-0.025741</td>\n",
       "      <td>-0.010555</td>\n",
       "      <td>-0.007448</td>\n",
       "      <td>-0.018276</td>\n",
       "      <td>-0.008414</td>\n",
       "      <td>-0.012801</td>\n",
       "      <td>-0.070148</td>\n",
       "      <td>-0.116766</td>\n",
       "      <td>-0.087045</td>\n",
       "      <td>-0.019538</td>\n",
       "      <td>-0.096158</td>\n",
       "      <td>-0.0316</td>\n",
       "      <td>-0.184193</td>\n",
       "      <td>-0.185007</td>\n",
       "      <td>-0.027699</td>\n",
       "      <td>-0.185764</td>\n",
       "      <td>-0.043433</td>\n",
       "      <td>-0.322095</td>\n",
       "      <td>-0.272335</td>\n",
       "      <td>-0.107900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.292922</td>\n",
       "      <td>0.164247</td>\n",
       "      <td>0.075371</td>\n",
       "      <td>0.314758</td>\n",
       "      <td>0.113402</td>\n",
       "      <td>0.069740</td>\n",
       "      <td>0.090233</td>\n",
       "      <td>-0.949850</td>\n",
       "      <td>-0.949643</td>\n",
       "      <td>-0.949749</td>\n",
       "      <td>-0.755956</td>\n",
       "      <td>0.639129</td>\n",
       "      <td>2.292881</td>\n",
       "      <td>2.685367</td>\n",
       "      <td>0.133310</td>\n",
       "      <td>1.790923</td>\n",
       "      <td>2.297012</td>\n",
       "      <td>-0.243232</td>\n",
       "      <td>2.213042</td>\n",
       "      <td>0.466136</td>\n",
       "      <td>0.604128</td>\n",
       "      <td>0.553850</td>\n",
       "      <td>0.266818</td>\n",
       "      <td>-0.12952</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.02293</td>\n",
       "      <td>-0.022502</td>\n",
       "      <td>-2.132323</td>\n",
       "      <td>-0.727023</td>\n",
       "      <td>-0.029292</td>\n",
       "      <td>-2.020930</td>\n",
       "      <td>0.109453</td>\n",
       "      <td>-0.193291</td>\n",
       "      <td>-0.125939</td>\n",
       "      <td>-0.010191</td>\n",
       "      <td>-0.152663</td>\n",
       "      <td>-0.126731</td>\n",
       "      <td>-0.382317</td>\n",
       "      <td>-0.530772</td>\n",
       "      <td>-0.013206</td>\n",
       "      <td>-1.164785</td>\n",
       "      <td>-0.880229</td>\n",
       "      <td>-0.006492</td>\n",
       "      <td>1.217756</td>\n",
       "      <td>0.075549</td>\n",
       "      <td>0.870439</td>\n",
       "      <td>1.160093</td>\n",
       "      <td>-0.015823</td>\n",
       "      <td>0.277125</td>\n",
       "      <td>0.000835</td>\n",
       "      <td>0.330911</td>\n",
       "      <td>0.312921</td>\n",
       "      <td>-0.016438</td>\n",
       "      <td>1.213877</td>\n",
       "      <td>0.075364</td>\n",
       "      <td>1.081787</td>\n",
       "      <td>1.168391</td>\n",
       "      <td>-0.916062</td>\n",
       "      <td>0.699170</td>\n",
       "      <td>0.108888</td>\n",
       "      <td>-0.263074</td>\n",
       "      <td>-0.024226</td>\n",
       "      <td>-0.010555</td>\n",
       "      <td>-0.007448</td>\n",
       "      <td>-0.018276</td>\n",
       "      <td>-0.008414</td>\n",
       "      <td>-0.012801</td>\n",
       "      <td>-0.070148</td>\n",
       "      <td>1.943266</td>\n",
       "      <td>1.092947</td>\n",
       "      <td>0.127038</td>\n",
       "      <td>1.450015</td>\n",
       "      <td>-0.0316</td>\n",
       "      <td>1.287788</td>\n",
       "      <td>1.216235</td>\n",
       "      <td>0.064961</td>\n",
       "      <td>1.253793</td>\n",
       "      <td>-0.043433</td>\n",
       "      <td>3.119271</td>\n",
       "      <td>2.404803</td>\n",
       "      <td>-0.037653</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num__id.orig_p  num__id.resp_p  num__flow_duration  num__fwd_pkts_tot  \\\n",
       "0       -1.460811       -0.189327           -0.028333          -0.052978   \n",
       "1       -0.745200       -0.189327           -0.028333          -0.052978   \n",
       "2        0.479444       -0.189327           -0.028333          -0.052978   \n",
       "3        1.126457       -0.189327           -0.028333          -0.052978   \n",
       "4        0.292922        0.164247            0.075371           0.314758   \n",
       "\n",
       "   num__bwd_pkts_tot  num__fwd_data_pkts_tot  num__bwd_data_pkts_tot  \\\n",
       "0          -0.025377               -0.022892               -0.023265   \n",
       "1          -0.025377               -0.022892               -0.023265   \n",
       "2          -0.025377               -0.022892               -0.023265   \n",
       "3          -0.053133               -0.022892               -0.023265   \n",
       "4           0.113402                0.069740                0.090233   \n",
       "\n",
       "   num__fwd_pkts_per_sec  num__bwd_pkts_per_sec  num__flow_pkts_per_sec  \\\n",
       "0              -0.285354              -0.285211               -0.285284   \n",
       "1              -0.411925              -0.411770               -0.411849   \n",
       "2              -0.411925              -0.411770               -0.411849   \n",
       "3              -0.949851              -0.949644               -0.949750   \n",
       "4              -0.949850              -0.949643               -0.949749   \n",
       "\n",
       "   num__down_up_ratio  num__fwd_header_size_tot  num__fwd_header_size_min  \\\n",
       "0            0.430173                 -0.080370                  0.043596   \n",
       "1            0.430173                 -0.080370                  0.043596   \n",
       "2            0.430173                 -0.080370                  0.043596   \n",
       "3           -2.535151                 -0.080370                  0.043596   \n",
       "4           -0.755956                  0.639129                  2.292881   \n",
       "\n",
       "   num__fwd_header_size_max  num__bwd_header_size_tot  \\\n",
       "0                 -0.087214                 -0.024031   \n",
       "1                 -0.087214                 -0.024031   \n",
       "2                 -0.087214                 -0.024031   \n",
       "3                 -0.087214                 -0.041513   \n",
       "4                  2.685367                  0.133310   \n",
       "\n",
       "   num__bwd_header_size_min  num__bwd_header_size_max  \\\n",
       "0                  0.288595                  0.167426   \n",
       "1                  0.288595                  0.167426   \n",
       "2                  0.288595                  0.167426   \n",
       "3                 -2.215285                 -1.962161   \n",
       "4                  1.790923                  2.297012   \n",
       "\n",
       "   num__flow_FIN_flag_count  num__flow_SYN_flag_count  \\\n",
       "0                 -0.243232                  0.105451   \n",
       "1                 -0.243232                  0.105451   \n",
       "2                 -0.243232                  0.105451   \n",
       "3                 -0.243232                  0.105451   \n",
       "4                 -0.243232                  2.213042   \n",
       "\n",
       "   num__flow_RST_flag_count  num__fwd_PSH_flag_count  num__bwd_PSH_flag_count  \\\n",
       "0                  0.466136                -0.082591                -0.061478   \n",
       "1                  0.466136                -0.082591                -0.061478   \n",
       "2                  0.466136                -0.082591                -0.061478   \n",
       "3                 -1.823694                -0.082591                -0.061478   \n",
       "4                  0.466136                 0.604128                 0.553850   \n",
       "\n",
       "   num__flow_ACK_flag_count  num__fwd_URG_flag_count  num__bwd_URG_flag_count  \\\n",
       "0                 -0.037392                 -0.12952                      0.0   \n",
       "1                 -0.037392                 -0.12952                      0.0   \n",
       "2                 -0.037392                 -0.12952                      0.0   \n",
       "3                 -0.059121                 -0.12952                      0.0   \n",
       "4                  0.266818                 -0.12952                      0.0   \n",
       "\n",
       "   num__flow_CWR_flag_count  num__flow_ECE_flag_count  \\\n",
       "0                  -0.02293                 -0.022502   \n",
       "1                  -0.02293                 -0.022502   \n",
       "2                  -0.02293                 -0.022502   \n",
       "3                  -0.02293                 -0.022502   \n",
       "4                  -0.02293                 -0.022502   \n",
       "\n",
       "   num__fwd_pkts_payload.min  num__fwd_pkts_payload.max  \\\n",
       "0                   0.525072                  -0.004776   \n",
       "1                   0.525072                  -0.004776   \n",
       "2                   0.525072                  -0.004776   \n",
       "3                   0.525072                  -0.004776   \n",
       "4                  -2.132323                  -0.727023   \n",
       "\n",
       "   num__fwd_pkts_payload.tot  num__fwd_pkts_payload.avg  \\\n",
       "0                  -0.020934                   0.424667   \n",
       "1                  -0.020934                   0.424667   \n",
       "2                  -0.020934                   0.424667   \n",
       "3                  -0.020934                   0.424667   \n",
       "4                  -0.029292                  -2.020930   \n",
       "\n",
       "   num__fwd_pkts_payload.std  num__bwd_pkts_payload.min  \\\n",
       "0                  -0.179677                  -0.193291   \n",
       "1                  -0.179677                  -0.193291   \n",
       "2                  -0.179677                  -0.193291   \n",
       "3                  -0.179677                  -0.193291   \n",
       "4                   0.109453                  -0.193291   \n",
       "\n",
       "   num__bwd_pkts_payload.max  num__bwd_pkts_payload.tot  \\\n",
       "0                  -0.226052                  -0.010964   \n",
       "1                  -0.226052                  -0.010964   \n",
       "2                  -0.226052                  -0.010964   \n",
       "3                  -0.226052                  -0.010964   \n",
       "4                  -0.125939                  -0.010191   \n",
       "\n",
       "   num__bwd_pkts_payload.avg  num__bwd_pkts_payload.std  \\\n",
       "0                  -0.225570                  -0.219344   \n",
       "1                  -0.225570                  -0.219344   \n",
       "2                  -0.225570                  -0.219344   \n",
       "3                  -0.225570                  -0.219344   \n",
       "4                  -0.152663                  -0.126731   \n",
       "\n",
       "   num__flow_pkts_payload.min  num__flow_pkts_payload.max  \\\n",
       "0                   -0.382317                   -0.129843   \n",
       "1                   -0.382317                   -0.129843   \n",
       "2                   -0.382317                   -0.129843   \n",
       "3                    3.008303                   -0.129843   \n",
       "4                   -0.382317                   -0.530772   \n",
       "\n",
       "   num__flow_pkts_payload.tot  num__flow_pkts_payload.avg  \\\n",
       "0                   -0.013079                   -0.098209   \n",
       "1                   -0.013079                   -0.098209   \n",
       "2                   -0.013079                   -0.098209   \n",
       "3                   -0.013079                    1.112089   \n",
       "4                   -0.013206                   -1.164785   \n",
       "\n",
       "   num__flow_pkts_payload.std  num__fwd_iat.min  num__fwd_iat.max  \\\n",
       "0                    0.121374         -0.006746         -0.187666   \n",
       "1                    0.121374         -0.006746         -0.187666   \n",
       "2                    0.121374         -0.006746         -0.187666   \n",
       "3                   -1.032825         -0.006746         -0.187666   \n",
       "4                   -0.880229         -0.006492          1.217756   \n",
       "\n",
       "   num__fwd_iat.tot  num__fwd_iat.avg  num__fwd_iat.std  num__bwd_iat.min  \\\n",
       "0         -0.028171         -0.139649         -0.183810         -0.016119   \n",
       "1         -0.028171         -0.139649         -0.183810         -0.016119   \n",
       "2         -0.028171         -0.139649         -0.183810         -0.016119   \n",
       "3         -0.028171         -0.139649         -0.183810         -0.016119   \n",
       "4          0.075549          0.870439          1.160093         -0.015823   \n",
       "\n",
       "   num__bwd_iat.max  num__bwd_iat.tot  num__bwd_iat.avg  num__bwd_iat.std  \\\n",
       "0         -0.098717         -0.019397         -0.088242         -0.090747   \n",
       "1         -0.098717         -0.019397         -0.088242         -0.090747   \n",
       "2         -0.098717         -0.019397         -0.088242         -0.090747   \n",
       "3         -0.098717         -0.019397         -0.088242         -0.090747   \n",
       "4          0.277125          0.000835          0.330911          0.312921   \n",
       "\n",
       "   num__flow_iat.min  num__flow_iat.max  num__flow_iat.tot  num__flow_iat.avg  \\\n",
       "0          -0.016711          -0.187684          -0.028340          -0.170172   \n",
       "1          -0.016707          -0.187684          -0.028340          -0.170170   \n",
       "2          -0.016707          -0.187684          -0.028340          -0.170170   \n",
       "3          -0.016728          -0.187684          -0.028340          -0.170177   \n",
       "4          -0.016438           1.213877           0.075364           1.081787   \n",
       "\n",
       "   num__flow_iat.std  num__payload_bytes_per_second  num__fwd_subflow_pkts  \\\n",
       "0          -0.183772                      -0.256986              -0.211362   \n",
       "1          -0.183772                      -0.382525              -0.211362   \n",
       "2          -0.183772                      -0.382525              -0.211362   \n",
       "3          -0.183772                      -0.916062              -0.211362   \n",
       "4           1.168391                      -0.916062               0.699170   \n",
       "\n",
       "   num__bwd_subflow_pkts  num__fwd_subflow_bytes  num__bwd_subflow_bytes  \\\n",
       "0              -0.052427               -0.037626               -0.025741   \n",
       "1              -0.052427               -0.037626               -0.025741   \n",
       "2              -0.052427               -0.037626               -0.025741   \n",
       "3              -0.213743               -0.037626               -0.025741   \n",
       "4               0.108888               -0.263074               -0.024226   \n",
       "\n",
       "   num__fwd_bulk_bytes  num__bwd_bulk_bytes  num__fwd_bulk_packets  \\\n",
       "0            -0.010555            -0.007448              -0.018276   \n",
       "1            -0.010555            -0.007448              -0.018276   \n",
       "2            -0.010555            -0.007448              -0.018276   \n",
       "3            -0.010555            -0.007448              -0.018276   \n",
       "4            -0.010555            -0.007448              -0.018276   \n",
       "\n",
       "   num__bwd_bulk_packets  num__fwd_bulk_rate  num__bwd_bulk_rate  \\\n",
       "0              -0.008414           -0.012801           -0.070148   \n",
       "1              -0.008414           -0.012801           -0.070148   \n",
       "2              -0.008414           -0.012801           -0.070148   \n",
       "3              -0.008414           -0.012801           -0.070148   \n",
       "4              -0.008414           -0.012801           -0.070148   \n",
       "\n",
       "   num__active.min  num__active.max  num__active.tot  num__active.avg  \\\n",
       "0        -0.116763        -0.087043        -0.019538        -0.096155   \n",
       "1        -0.116762        -0.087043        -0.019538        -0.096154   \n",
       "2        -0.116762        -0.087043        -0.019538        -0.096154   \n",
       "3        -0.116766        -0.087045        -0.019538        -0.096158   \n",
       "4         1.943266         1.092947         0.127038         1.450015   \n",
       "\n",
       "   num__active.std  num__idle.min  num__idle.max  num__idle.tot  \\\n",
       "0          -0.0316      -0.184193      -0.185007      -0.027699   \n",
       "1          -0.0316      -0.184193      -0.185007      -0.027699   \n",
       "2          -0.0316      -0.184193      -0.185007      -0.027699   \n",
       "3          -0.0316      -0.184193      -0.185007      -0.027699   \n",
       "4          -0.0316       1.287788       1.216235       0.064961   \n",
       "\n",
       "   num__idle.avg  num__idle.std  num__fwd_init_window_size  \\\n",
       "0      -0.185764      -0.043433                  -0.322095   \n",
       "1      -0.185764      -0.043433                  -0.322095   \n",
       "2      -0.185764      -0.043433                  -0.322095   \n",
       "3      -0.185764      -0.043433                  -0.322095   \n",
       "4       1.253793      -0.043433                   3.119271   \n",
       "\n",
       "   num__bwd_init_window_size  num__fwd_last_window_size  cat__proto_tcp  \\\n",
       "0                  -0.272335                  -0.107900             1.0   \n",
       "1                  -0.272335                  -0.107900             1.0   \n",
       "2                  -0.272335                  -0.107900             1.0   \n",
       "3                  -0.272335                  -0.107900             1.0   \n",
       "4                   2.404803                  -0.037653             1.0   \n",
       "\n",
       "   cat__proto_udp  \n",
       "0             0.0  \n",
       "1             0.0  \n",
       "2             0.0  \n",
       "3             0.0  \n",
       "4             0.0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 다중 분류용 전처리 적용 후 데이터 확인\n",
    "features_names_multi = preprocessor.get_feature_names_out()\n",
    "X_processed_multi = pd.DataFrame(X_train_multi_processed, columns=features_names_multi)\n",
    "X_processed_multi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.6.0-py3-none-win_amd64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\seren\\anaconda3\\lib\\site-packages (from lightgbm) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\seren\\anaconda3\\lib\\site-packages (from lightgbm) (1.13.1)\n",
      "Downloading lightgbm-4.6.0-py3-none-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 19.2 MB/s eta 0:00:00\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 다중분류모델 XGBoost 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\seren\\anaconda3\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\seren\\anaconda3\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\seren\\anaconda3\\lib\\site-packages (from xgboost) (1.13.1)\n",
      "Collecting hyperopt\n",
      "  Downloading hyperopt-0.2.7-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\seren\\anaconda3\\lib\\site-packages (from hyperopt) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\seren\\anaconda3\\lib\\site-packages (from hyperopt) (1.13.1)\n",
      "Requirement already satisfied: six in c:\\users\\seren\\anaconda3\\lib\\site-packages (from hyperopt) (1.16.0)\n",
      "Requirement already satisfied: networkx>=2.2 in c:\\users\\seren\\anaconda3\\lib\\site-packages (from hyperopt) (3.3)\n",
      "Collecting future (from hyperopt)\n",
      "  Downloading future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\seren\\anaconda3\\lib\\site-packages (from hyperopt) (4.66.5)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\seren\\anaconda3\\lib\\site-packages (from hyperopt) (3.0.0)\n",
      "Collecting py4j (from hyperopt)\n",
      "  Downloading py4j-0.10.9.9-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\seren\\anaconda3\\lib\\site-packages (from tqdm->hyperopt) (0.4.6)\n",
      "Downloading hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.6/1.6 MB 16.9 MB/s eta 0:00:00\n",
      "Downloading future-1.0.0-py3-none-any.whl (491 kB)\n",
      "Downloading py4j-0.10.9.9-py2.py3-none-any.whl (203 kB)\n",
      "Installing collected packages: py4j, future, hyperopt\n",
      "Successfully installed future-1.0.0 hyperopt-0.2.7 py4j-0.10.9.9\n",
      "  0%|          | 0/10 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\seren\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [09:44:24] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "\n",
      "c:\\Users\\seren\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [09:45:57] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "\n",
      "c:\\Users\\seren\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [09:47:46] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [05:03<45:27, 303.10s/trial, best loss: -0.9990537211844588]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\seren\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [09:49:28] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "\n",
      "c:\\Users\\seren\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [09:51:32] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "\n",
      "c:\\Users\\seren\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [09:53:35] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [11:22<46:26, 348.25s/trial, best loss: -0.9990537211844588]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\seren\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [09:55:47] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "\n",
      "c:\\Users\\seren\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [09:56:52] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "\n",
      "c:\\Users\\seren\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [09:57:52] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [14:19<31:28, 269.73s/trial, best loss: -0.9990537211844588]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\seren\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [09:58:43] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "\n",
      "c:\\Users\\seren\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [09:59:54] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "\n",
      "c:\\Users\\seren\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [10:01:04] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [18:00<25:02, 250.48s/trial, best loss: -0.9993427419202449]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\seren\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [10:02:26] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "\n",
      "c:\\Users\\seren\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [10:03:39] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "\n",
      "c:\\Users\\seren\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [10:04:50] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [21:39<19:56, 239.31s/trial, best loss: -0.9993427419202449]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\seren\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [10:06:05] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "\n",
      "c:\\Users\\seren\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [10:07:14] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "\n",
      "c:\\Users\\seren\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [10:08:09] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [24:34<14:29, 217.26s/trial, best loss: -0.9993427419202449]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\seren\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [10:08:58] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "\n",
      "c:\\Users\\seren\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [10:10:58] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "\n",
      "c:\\Users\\seren\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [10:13:00] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [30:47<13:25, 268.43s/trial, best loss: -0.9993427419202449]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\seren\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [10:15:12] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "\n",
      "c:\\Users\\seren\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [10:18:06] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "\n",
      "c:\\Users\\seren\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [10:20:37] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [38:42<11:07, 333.92s/trial, best loss: -0.9993427419202449]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\seren\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [10:23:06] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "\n",
      "c:\\Users\\seren\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [10:25:05] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "\n",
      "c:\\Users\\seren\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [10:27:05] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [44:44<05:42, 342.91s/trial, best loss: -0.9993427419202449]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\seren\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [10:29:09] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "\n",
      "c:\\Users\\seren\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [10:30:56] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "\n",
      "c:\\Users\\seren\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [10:32:54] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [49:49<00:00, 298.98s/trial, best loss: -0.9993427419202449]\n",
      "Best hyperparameters found:\n",
      "{'colsample_bytree': 0.8502251829685219, 'gamma': 0.36958272662759584, 'learning_rate': 0.14384740309225888, 'max_depth': 7.0, 'n_estimators': 80.0, 'subsample': 0.8211333010785937}\n"
     ]
    }
   ],
   "source": [
    "# 설치 필요 시\n",
    "!pip install xgboost\n",
    "!pip install hyperopt\n",
    "\n",
    "from hyperopt import hp, fmin, tpe, Trials\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "\n",
    "# 1. 탐색 공간 정의 (XGBoost 하이퍼파라미터)\n",
    "xgb_search_space = {\n",
    "    'max_depth': hp.quniform('max_depth', 3, 10, 1),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
    "    'n_estimators': hp.quniform('n_estimators', 50, 200, 10),\n",
    "    'subsample': hp.uniform('subsample', 0.7, 1.0),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.7, 1.0),\n",
    "    'gamma': hp.uniform('gamma', 0, 5)\n",
    "}\n",
    "\n",
    "# 2. 목적 함수 정의\n",
    "def objective_func(params):\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "    params['n_estimators'] = int(params['n_estimators'])\n",
    "\n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    f1_scores = []\n",
    "\n",
    "    for train_index, val_index in kf.split(X_train_multi_resampled):\n",
    "        X_tr = X_train_multi_resampled[train_index]\n",
    "        y_tr = y_train_multi_resampled[train_index]\n",
    "        X_val = X_train_multi_resampled[val_index]\n",
    "        y_val = y_train_multi_resampled[val_index]\n",
    "\n",
    "        model = XGBClassifier(\n",
    "            objective='multi:softmax',\n",
    "            num_class=len(np.unique(y_train_multi_resampled)),\n",
    "            use_label_encoder=False,\n",
    "            eval_metric='mlogloss',\n",
    "            random_state=42,\n",
    "            **params\n",
    "        )\n",
    "\n",
    "        model.fit(X_tr, y_tr)\n",
    "        preds = model.predict(X_val)\n",
    "        f1 = f1_score(y_val, preds, average='weighted')\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    return -np.mean(f1_scores)\n",
    "\n",
    "# 3. 하이퍼파라미터 튜닝 실행\n",
    "trials = Trials()\n",
    "best = fmin(\n",
    "    fn=objective_func,\n",
    "    space=xgb_search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=10,\n",
    "    trials=trials,\n",
    "    rstate=np.random.default_rng(seed=42)\n",
    ")\n",
    "\n",
    "print(\"Best hyperparameters found:\")\n",
    "print(best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\seren\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [12:39:24] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"class_weight\", \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 XGBoost 모델 평가 결과:\n",
      "[[ 1568     0     0     0     0     0     0     2     0     8]\n",
      " [    0   100     0     0     0     0     0     0     0     0]\n",
      " [    0     0 18897     0     0     0     0     0     0     0]\n",
      " [    0     0     0     4     0     0     0     2     0     0]\n",
      " [    1     0     0     0     2     0     0     0     0     0]\n",
      " [    0     0     0     0     0   393     0     0     0     0]\n",
      " [    0     0     0     0     0     0   220     0     0     0]\n",
      " [    3     0     0     0     0     0     0   484     0     2]\n",
      " [    2     0     0     0     0     0     0     1   381     0]\n",
      " [   17     1     0     0     0     0     0     0     0  2536]]\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "            ARP_poisioning       0.99      0.99      0.99      1578\n",
      "            DDOS_Slowloris       0.99      1.00      1.00       100\n",
      "             DOS_SYN_Hping       1.00      1.00      1.00     18897\n",
      "Metasploit_Brute_Force_SSH       1.00      0.67      0.80         6\n",
      "             NMAP_FIN_SCAN       1.00      0.67      0.80         3\n",
      "         NMAP_OS_DETECTION       1.00      1.00      1.00       393\n",
      "             NMAP_TCP_scan       1.00      1.00      1.00       220\n",
      "             NMAP_UDP_SCAN       0.99      0.99      0.99       489\n",
      "       NMAP_XMAS_TREE_SCAN       1.00      0.99      1.00       384\n",
      "                    Normal       1.00      0.99      0.99      2554\n",
      "\n",
      "                  accuracy                           1.00     24624\n",
      "                 macro avg       1.00      0.93      0.96     24624\n",
      "              weighted avg       1.00      1.00      1.00     24624\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# best 딕셔너리 값 변환\n",
    "best_params_final = {\n",
    "    'learning_rate': best['learning_rate'],\n",
    "    'max_depth': int(best['max_depth']),\n",
    "    'n_estimators': int(best['n_estimators']),\n",
    "    'subsample': best['subsample'],\n",
    "    'colsample_bytree': best['colsample_bytree'],\n",
    "    'gamma': best['gamma']\n",
    "}\n",
    "\n",
    "# 최종 XGBoost 모델 정의\n",
    "final_model = XGBClassifier(\n",
    "    objective='multi:softmax',\n",
    "    num_class=len(np.unique(y_train_multi_resampled)),\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='mlogloss',\n",
    "    class_weight='balanced',  # XGBoost는 class_weight 지원 X → 무시됨\n",
    "    random_state=42,\n",
    "    **best_params_final\n",
    ")\n",
    "\n",
    "# 모델 학습\n",
    "final_model.fit(X_train_multi_resampled, y_train_multi_resampled)\n",
    "\n",
    "# 예측\n",
    "y_pred_final = final_model.predict(X_test_multi_processed)\n",
    "\n",
    "# 평가\n",
    "print(\"최종 XGBoost 모델 평가 결과:\")\n",
    "print(confusion_matrix(y_test_multi, y_pred_final))\n",
    "print(classification_report(y_test_multi, y_pred_final, target_names=multi_class_label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 이진 분류 모델-lightGBM 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 59056, number of negative: 58997\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.115338 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18570                    \n",
      "[LightGBM] [Info] Number of data points in the train set: 118053, number of used features: 82\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000 \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Did not meet early stopping. Best iteration is:       \n",
      "[93]\tvalid_0's binary_logloss: 0.00231717\n",
      "[LightGBM] [Info] Number of positive: 58962, number of negative: 59091\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048845 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18437                    \n",
      "[LightGBM] [Info] Number of data points in the train set: 118053, number of used features: 82\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000  \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:       \n",
      "[93]\tvalid_0's binary_logloss: 0.00221581\n",
      "[LightGBM] [Info] Number of positive: 59062, number of negative: 58992\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045402 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18282                    \n",
      "[LightGBM] [Info] Number of data points in the train set: 118054, number of used features: 82\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000  \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:       \n",
      "[97]\tvalid_0's binary_logloss: 0.00225863\n",
      "[LightGBM] [Info] Number of positive: 59056, number of negative: 58997           \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.081092 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18570                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 118053, number of used features: 82\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000  \n",
      "[LightGBM] [Info] Start training from score -0.000000                            \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "Training until validation scores don't improve for 30 rounds                     \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "Did not meet early stopping. Best iteration is:                                  \n",
      "[100]\tvalid_0's binary_logloss: 0.00362158\n",
      "[LightGBM] [Info] Number of positive: 58962, number of negative: 59091           \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.088220 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18437                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 118053, number of used features: 82\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000   \n",
      "[LightGBM] [Info] Start training from score 0.000000                             \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "Training until validation scores don't improve for 30 rounds                     \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "Did not meet early stopping. Best iteration is:                                  \n",
      "[100]\tvalid_0's binary_logloss: 0.00368017\n",
      "[LightGBM] [Info] Number of positive: 59062, number of negative: 58992           \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067325 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18282                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 118054, number of used features: 82\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000   \n",
      "[LightGBM] [Info] Start training from score 0.000000                             \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "Training until validation scores don't improve for 30 rounds                     \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "Did not meet early stopping. Best iteration is:                                  \n",
      "[100]\tvalid_0's binary_logloss: 0.00349134\n",
      "[LightGBM] [Info] Number of positive: 59056, number of negative: 58997           \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058038 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18570                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 118053, number of used features: 82\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000  \n",
      "[LightGBM] [Info] Start training from score -0.000000                            \n",
      "Training until validation scores don't improve for 30 rounds                     \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "Did not meet early stopping. Best iteration is:                                  \n",
      "[100]\tvalid_0's binary_logloss: 0.00302223\n",
      "[LightGBM] [Info] Number of positive: 58962, number of negative: 59091           \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047427 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18437                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 118053, number of used features: 82\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000   \n",
      "[LightGBM] [Info] Start training from score 0.000000                             \n",
      "Training until validation scores don't improve for 30 rounds                     \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "Did not meet early stopping. Best iteration is:                                  \n",
      "[100]\tvalid_0's binary_logloss: 0.00322745\n",
      "[LightGBM] [Info] Number of positive: 59062, number of negative: 58992           \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046024 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18282                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 118054, number of used features: 82\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000   \n",
      "[LightGBM] [Info] Start training from score 0.000000                             \n",
      "Training until validation scores don't improve for 30 rounds                     \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "Did not meet early stopping. Best iteration is:                                  \n",
      "[100]\tvalid_0's binary_logloss: 0.0032388\n",
      "[LightGBM] [Info] Number of positive: 59056, number of negative: 58997           \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043798 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18570                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 118053, number of used features: 82\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000  \n",
      "[LightGBM] [Info] Start training from score -0.000000                            \n",
      "Training until validation scores don't improve for 30 rounds                     \n",
      "Did not meet early stopping. Best iteration is:                                  \n",
      "[100]\tvalid_0's binary_logloss: 0.0523587\n",
      "[LightGBM] [Info] Number of positive: 58962, number of negative: 59091           \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043755 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18437                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 118053, number of used features: 82\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000   \n",
      "[LightGBM] [Info] Start training from score 0.000000                             \n",
      "Training until validation scores don't improve for 30 rounds                     \n",
      "Did not meet early stopping. Best iteration is:                                  \n",
      "[100]\tvalid_0's binary_logloss: 0.0523306\n",
      "[LightGBM] [Info] Number of positive: 59062, number of negative: 58992           \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037782 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 18282                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 118054, number of used features: 82\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000   \n",
      "[LightGBM] [Info] Start training from score 0.000000                             \n",
      "Training until validation scores don't improve for 30 rounds                     \n",
      "Did not meet early stopping. Best iteration is:                                  \n",
      "[100]\tvalid_0's binary_logloss: 0.051238\n",
      "[LightGBM] [Info] Number of positive: 59056, number of negative: 58997           \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057415 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18570                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 118053, number of used features: 82\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000  \n",
      "[LightGBM] [Info] Start training from score -0.000000                            \n",
      "Training until validation scores don't improve for 30 rounds                     \n",
      "Did not meet early stopping. Best iteration is:                                  \n",
      "[100]\tvalid_0's binary_logloss: 0.00357522\n",
      "[LightGBM] [Info] Number of positive: 58962, number of negative: 59091           \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053205 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18437                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 118053, number of used features: 82\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000   \n",
      "[LightGBM] [Info] Start training from score 0.000000                             \n",
      "Training until validation scores don't improve for 30 rounds                     \n",
      "Did not meet early stopping. Best iteration is:                                  \n",
      "[100]\tvalid_0's binary_logloss: 0.00401058\n",
      "[LightGBM] [Info] Number of positive: 59062, number of negative: 58992           \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.103901 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18282                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 118054, number of used features: 82\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000   \n",
      "[LightGBM] [Info] Start training from score 0.000000                             \n",
      "Training until validation scores don't improve for 30 rounds                     \n",
      "Did not meet early stopping. Best iteration is:                                  \n",
      "[100]\tvalid_0's binary_logloss: 0.00354276\n",
      "[LightGBM] [Info] Number of positive: 59056, number of negative: 58997           \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061131 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18570                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 118053, number of used features: 82\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000  \n",
      "[LightGBM] [Info] Start training from score -0.000000                            \n",
      "Training until validation scores don't improve for 30 rounds                     \n",
      "Did not meet early stopping. Best iteration is:                                  \n",
      "[81]\tvalid_0's binary_logloss: 0.00230402\n",
      "[LightGBM] [Info] Number of positive: 58962, number of negative: 59091           \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053440 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18437                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 118053, number of used features: 82\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000   \n",
      "[LightGBM] [Info] Start training from score 0.000000                             \n",
      "Training until validation scores don't improve for 30 rounds                     \n",
      "Did not meet early stopping. Best iteration is:                                  \n",
      "[85]\tvalid_0's binary_logloss: 0.00223518\n",
      "[LightGBM] [Info] Number of positive: 59062, number of negative: 58992           \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044963 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 18282                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 118054, number of used features: 82\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000   \n",
      "[LightGBM] [Info] Start training from score 0.000000                             \n",
      "Training until validation scores don't improve for 30 rounds                     \n",
      "Did not meet early stopping. Best iteration is:                                  \n",
      "[84]\tvalid_0's binary_logloss: 0.00238662\n",
      "[LightGBM] [Info] Number of positive: 59056, number of negative: 58997           \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048698 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18570                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 118053, number of used features: 82\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000  \n",
      "[LightGBM] [Info] Start training from score -0.000000                            \n",
      "Training until validation scores don't improve for 30 rounds                     \n",
      "Did not meet early stopping. Best iteration is:                                  \n",
      "[98]\tvalid_0's binary_logloss: 0.0025571\n",
      "[LightGBM] [Info] Number of positive: 58962, number of negative: 59091           \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051700 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18437                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 118053, number of used features: 82\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000   \n",
      "[LightGBM] [Info] Start training from score 0.000000                             \n",
      "Training until validation scores don't improve for 30 rounds                     \n",
      "Did not meet early stopping. Best iteration is:                                  \n",
      "[99]\tvalid_0's binary_logloss: 0.00265873\n",
      "[LightGBM] [Info] Number of positive: 59062, number of negative: 58992           \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.133588 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18282                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 118054, number of used features: 82\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000   \n",
      "[LightGBM] [Info] Start training from score 0.000000                             \n",
      "Training until validation scores don't improve for 30 rounds                     \n",
      "Did not meet early stopping. Best iteration is:                                  \n",
      "[93]\tvalid_0's binary_logloss: 0.00266847\n",
      "[LightGBM] [Info] Number of positive: 59056, number of negative: 58997           \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050915 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18570                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 118053, number of used features: 82\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000  \n",
      "[LightGBM] [Info] Start training from score -0.000000                            \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "Training until validation scores don't improve for 30 rounds                     \n",
      "Did not meet early stopping. Best iteration is:                                  \n",
      "[100]\tvalid_0's binary_logloss: 0.018276\n",
      "[LightGBM] [Info] Number of positive: 58962, number of negative: 59091           \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051407 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18437                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 118053, number of used features: 82\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000   \n",
      "[LightGBM] [Info] Start training from score 0.000000                             \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "Training until validation scores don't improve for 30 rounds                     \n",
      "Did not meet early stopping. Best iteration is:                                  \n",
      "[100]\tvalid_0's binary_logloss: 0.0183797\n",
      "[LightGBM] [Info] Number of positive: 59062, number of negative: 58992           \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059217 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18282                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 118054, number of used features: 82\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000   \n",
      "[LightGBM] [Info] Start training from score 0.000000                             \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "Training until validation scores don't improve for 30 rounds                     \n",
      "Did not meet early stopping. Best iteration is:                                  \n",
      "[100]\tvalid_0's binary_logloss: 0.0175681\n",
      "[LightGBM] [Info] Number of positive: 59056, number of negative: 58997           \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053784 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18570                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 118053, number of used features: 82\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000  \n",
      "[LightGBM] [Info] Start training from score -0.000000                            \n",
      "Training until validation scores don't improve for 30 rounds                     \n",
      "Did not meet early stopping. Best iteration is:                                  \n",
      "[75]\tvalid_0's binary_logloss: 0.0025986\n",
      "[LightGBM] [Info] Number of positive: 58962, number of negative: 59091           \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021017 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 18437                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 118053, number of used features: 82\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000   \n",
      "[LightGBM] [Info] Start training from score 0.000000                             \n",
      "Training until validation scores don't improve for 30 rounds                     \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "Did not meet early stopping. Best iteration is:                                  \n",
      "[75]\tvalid_0's binary_logloss: 0.0026718\n",
      "[LightGBM] [Info] Number of positive: 59062, number of negative: 58992           \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060900 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18282                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 118054, number of used features: 82\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000   \n",
      "[LightGBM] [Info] Start training from score 0.000000                             \n",
      "Training until validation scores don't improve for 30 rounds                     \n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n",
      "Did not meet early stopping. Best iteration is:                                  \n",
      "[74]\tvalid_0's binary_logloss: 0.00234319\n",
      "[LightGBM] [Info] Number of positive: 59056, number of negative: 58997           \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050548 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18570                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 118053, number of used features: 82\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000  \n",
      "[LightGBM] [Info] Start training from score -0.000000                            \n",
      "Training until validation scores don't improve for 30 rounds                     \n",
      "Did not meet early stopping. Best iteration is:                                  \n",
      "[100]\tvalid_0's binary_logloss: 0.00227525\n",
      "[LightGBM] [Info] Number of positive: 58962, number of negative: 59091           \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.176484 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18437                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 118053, number of used features: 82\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000   \n",
      "[LightGBM] [Info] Start training from score 0.000000                             \n",
      "Training until validation scores don't improve for 30 rounds                     \n",
      "Did not meet early stopping. Best iteration is:                                  \n",
      "[95]\tvalid_0's binary_logloss: 0.00231792\n",
      "[LightGBM] [Info] Number of positive: 59062, number of negative: 58992           \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056207 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18282                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 118054, number of used features: 82\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000   \n",
      "[LightGBM] [Info] Start training from score 0.000000                             \n",
      "Training until validation scores don't improve for 30 rounds                     \n",
      "Did not meet early stopping. Best iteration is:                                  \n",
      "[98]\tvalid_0's binary_logloss: 0.00248573\n",
      "100%|██████████| 10/10 [02:23<00:00, 14.33s/trial, best loss: -0.9993169382080521]\n",
      "Best hyperparameters found:\n",
      "{'learning_rate': 0.15589271557542145, 'max_depth': 15.0, 'min_child_samples': 91.0, 'num_leaves': 52.0, 'subsample': 0.9440966222920864}\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import hp, fmin, tpe, Trials\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from lightgbm import early_stopping, log_evaluation\n",
    "\n",
    "# 탐색 공간 정의\n",
    "lgbm_search_space = {\n",
    "    'num_leaves': hp.quniform('num_leaves', 32, 64, 1),\n",
    "    'max_depth': hp.quniform('max_depth', 5, 20, 1),\n",
    "    'min_child_samples': hp.quniform('min_child_samples', 20, 100, 1),\n",
    "    'subsample': hp.uniform('subsample', 0.7, 1.0),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2)\n",
    "}\n",
    "\n",
    "# 목적 함수 정의\n",
    "def objective_func(search_space):\n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    f1_scores = []\n",
    "\n",
    "    for train_index, val_index in kf.split(X_train_binary_resampled):\n",
    "        X_tr = X_train_binary_resampled[train_index]\n",
    "        y_tr = y_train_binary_resampled[train_index]\n",
    "        X_val = X_train_binary_resampled[val_index]\n",
    "        y_val = y_train_binary_resampled[val_index]\n",
    "\n",
    "        clf = LGBMClassifier(\n",
    "            objective='binary',\n",
    "            n_estimators=100,\n",
    "            num_leaves=int(search_space['num_leaves']),\n",
    "            max_depth=int(search_space['max_depth']),\n",
    "            min_child_samples=int(search_space['min_child_samples']),\n",
    "            subsample=search_space['subsample'],\n",
    "            learning_rate=search_space['learning_rate'],\n",
    "            class_weight='balanced',\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        clf.fit(\n",
    "            X_tr, y_tr,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_metric='binary_logloss',\n",
    "            callbacks=[\n",
    "                early_stopping(30),\n",
    "                log_evaluation(0)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        preds = clf.predict(X_val)\n",
    "        f1 = f1_score(y_val, preds)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    return -np.mean(f1_scores)  # 최소화\n",
    "\n",
    "# 튜닝 실행\n",
    "trials = Trials()\n",
    "best = fmin(\n",
    "    fn=objective_func,\n",
    "    space=lgbm_search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=10,  # 50 이상으로 늘릴 수도 있음\n",
    "    trials=trials,\n",
    "    rstate=np.random.default_rng(seed=42)\n",
    ")\n",
    "\n",
    "print(\"Best hyperparameters found:\")\n",
    "print(best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal과 Attack을 수동으로 균형 맞춰 추출\n",
    "df_normal = df[df['Binary_Attack_type'] == 'Normal']\n",
    "df_attack = df[df['Binary_Attack_type'] == 'Attack']\n",
    "\n",
    "# 공격 데이터를 Normal 수만큼 샘플링\n",
    "df_attack_balanced = df_attack.sample(n=len(df_normal), random_state=42)\n",
    "df_balanced = pd.concat([df_normal, df_attack_balanced]).sample(frac=1, random_state=42)\n",
    "\n",
    "# 이제 분리\n",
    "X = df_balanced.drop(columns=['Attack_type', 'Binary_Attack_type', 'Binary_Encoded'])\n",
    "y = df_balanced['Binary_Encoded']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 데이터 라벨 분포:\n",
      "Binary_Attack_type\n",
      "Attack    110610\n",
      "Normal     12507\n",
      "Name: count, dtype: int64\n",
      "Binary_Encoded\n",
      "0    110610\n",
      "1     12507\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Train 데이터 라벨 분포:\n",
      "Binary_Encoded\n",
      "0    88487\n",
      "1    10006\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test 데이터 라벨 분포:\n",
      "Binary_Encoded\n",
      "0    22123\n",
      "1     2501\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 전체 라벨 확인\n",
    "print(\"전체 데이터 라벨 분포:\")\n",
    "print(df['Binary_Attack_type'].value_counts())\n",
    "print(df['Binary_Encoded'].value_counts())\n",
    "\n",
    "# train/test split 전 stratify 확인\n",
    "X = df.drop(columns=['Attack_type', 'Binary_Attack_type', 'Binary_Encoded'])\n",
    "y = df['Binary_Encoded']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"\\nTrain 데이터 라벨 분포:\")\n",
    "print(y_train.value_counts())\n",
    "print(\"\\nTest 데이터 라벨 분포:\")\n",
    "print(y_test.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 88540, number of negative: 88540\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068638 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19027\n",
      "[LightGBM] [Info] Number of data points in the train set: 177080, number of used features: 82\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "\n",
      " [LightGBM 이진 분류 최종 평가 결과]\n",
      "[[19208  2862]\n",
      " [ 1279  1275]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.87      0.90     22070\n",
      "           1       0.31      0.50      0.38      2554\n",
      "\n",
      "    accuracy                           0.83     24624\n",
      "   macro avg       0.62      0.68      0.64     24624\n",
      "weighted avg       0.87      0.83      0.85     24624\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 최적 파라미터 기반 최종 모델 정의\n",
    "final_bin_model = LGBMClassifier(\n",
    "    objective='binary',\n",
    "    n_estimators=500,\n",
    "    learning_rate=best['learning_rate'],\n",
    "    max_depth=int(best['max_depth']),\n",
    "    min_child_samples=int(best['min_child_samples']),\n",
    "    num_leaves=int(best['num_leaves']),\n",
    "    subsample=best['subsample'],\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 모델 학습\n",
    "final_bin_model.fit(X_train_binary_resampled, y_train_binary_resampled)\n",
    "\n",
    "# 예측\n",
    "y_pred_final_bin = final_bin_model.predict(X_test_binary_processed)\n",
    "\n",
    "# 평가 출력\n",
    "print(\"\\n [LightGBM 이진 분류 최종 평가 결과]\")\n",
    "print(confusion_matrix(y_test_binary, y_pred_final_bin))\n",
    "print(classification_report(y_test_binary, y_pred_final_bin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
